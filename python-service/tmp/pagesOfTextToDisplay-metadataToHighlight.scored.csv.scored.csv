,paperID,pos_score,sent_pos,text
111,pagesOfTextToDisplay-metadataToHighlight.json,0.3866047263145447,111,"At the decoding stage, the spotlight module is first engaged to handle the ""where-to-look"" problem."
286,pagesOfTextToDisplay-metadataToHighlight.json,0.22133202850818634,286,"Moreover, all models reach their lowest validation loss before 30 epochs, with STNR and STNM both come to their best point earlier."
227,pagesOfTextToDisplay-metadataToHighlight.json,0.19963397085666656,227,5.1.2 STN setting.
232,pagesOfTextToDisplay-metadataToHighlight.json,0.19963397085666656,232,5.1.3 Training setting.
191,pagesOfTextToDisplay-metadataToHighlight.json,0.1941753625869751,191,b) Recurrent control module.
117,pagesOfTextToDisplay-metadataToHighlight.json,0.16701015830039978,117,in formula transcribing task.
21,pagesOfTextToDisplay-metadataToHighlight.json,0.15760868787765503,21,https://doi.
174,pagesOfTextToDisplay-metadataToHighlight.json,0.15760868787765503,174,coordinate.
280,pagesOfTextToDisplay-metadataToHighlight.json,0.10353658348321915,280,5.2.2 Validation loss.
225,pagesOfTextToDisplay-metadataToHighlight.json,0.07765626907348633,225,"From each training set, we also sample 10% images as validation set."
16,pagesOfTextToDisplay-metadataToHighlight.json,0.06721291691064835,16,Request permissions from permissions@acm.org.
238,pagesOfTextToDisplay-metadataToHighlight.json,0.06323693692684174,238,5.1.4 Comparison methods.
290,pagesOfTextToDisplay-metadataToHighlight.json,0.06248842552304268,290,5.2.3 Spotlight visualization.
301,pagesOfTextToDisplay-metadataToHighlight.json,0.062043994665145874,301,wrong prediction.
304,pagesOfTextToDisplay-metadataToHighlight.json,0.05514894798398018,304,5.2.4 Discussion.
272,pagesOfTextToDisplay-metadataToHighlight.json,0.040631793439388275,272,"From the results, we can get several observations."
148,pagesOfTextToDisplay-metadataToHighlight.json,0.0231536366045475,148, 1 1 1 . . .
76,pagesOfTextToDisplay-metadataToHighlight.json,0.02036927454173565,76,tokens per image Avg.
20,pagesOfTextToDisplay-metadataToHighlight.json,0.013453084044158459,20,"ACM, New York, NY, USA, 10 pages."
188,pagesOfTextToDisplay-metadataToHighlight.json,0.013174287043511868,188,STNR with Recurrent modeling.
18,pagesOfTextToDisplay-metadataToHighlight.json,0.010598444379866123,18,ACM ISBN 978-1-4503-5552-0/18/08. . .
269,pagesOfTextToDisplay-metadataToHighlight.json,0.009559717029333115,269,5.2 Experimental Results 5.2.1 Transcribing performance.
223,pagesOfTextToDisplay-metadataToHighlight.json,0.00866332184523344,223,5.1 Experimental Setup 5.1.1 Data partition and preprocessing.
253,pagesOfTextToDisplay-metadataToHighlight.json,0.007690094411373138,253,0 Loss Enc-Dec Attn-Dot Attn-FC Attn-Pos STNM STNR (a) Melody 0 5 10 15 20 25 Epochs 0 .
83,pagesOfTextToDisplay-metadataToHighlight.json,0.007514264900237322,83,"At last, we give the formal definition of the structural image transcription problem."
120,pagesOfTextToDisplay-metadataToHighlight.json,0.007290201261639595,120,Structural Image Transcription Problem ).
309,pagesOfTextToDisplay-metadataToHighlight.json,0.006475562695413828,309,"We are willing to utilize more prior knowledge, such as lexicons and hand-engineered features, to further improve the performance."
266,pagesOfTextToDisplay-metadataToHighlight.json,0.0051180217415094376,266,"Attn-Pos is an encoder-decoder model designed specifically for scene text recognition [ 37 ], where besides the image content, it also embeds location information into attention calculation, and get superior results."
82,pagesOfTextToDisplay-metadataToHighlight.json,0.0048482040874660015,82,Then we discuss the crucial differences between structural image transcribing and typical scene text recognition with exclusive data analysis.
259,pagesOfTextToDisplay-metadataToHighlight.json,0.0044985017739236355,259,0 Loss Enc-Dec Attn-Dot Attn-FC Attn-Pos STNM STNR (b) Formula 0 5 10 15 20 25 30 Epochs 0 .
246,pagesOfTextToDisplay-metadataToHighlight.json,0.004481846000999212,246,Table 2: Transcription accuracy on three datasets. (
156,pagesOfTextToDisplay-metadataToHighlight.json,0.00440767128020525,156,H ′ 1 2 . . .
183,pagesOfTextToDisplay-metadataToHighlight.json,0.004008651711046696,183,STNM with Markov property.
182,pagesOfTextToDisplay-metadataToHighlight.json,0.003998739644885063,182,Each implementation models the spotlight handle sequences differently.
12,pagesOfTextToDisplay-metadataToHighlight.json,0.003968613222241402,12,Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
287,pagesOfTextToDisplay-metadataToHighlight.json,0.0039198994636535645,287,"Thus, in our experiments, we train both STNR and STNM for 25, 15, 20 epochs on Melody, Formula and Multi-Line datasets respectively to obtain the best performance."
273,pagesOfTextToDisplay-metadataToHighlight.json,0.003891753265634179,273,"First, both STNM and STNR perform better than all the other methods."
233,pagesOfTextToDisplay-metadataToHighlight.json,0.0028634751215577126,233,"To set up the training process, we initialize all parameters in STN following [ 10 ]."
126,pagesOfTextToDisplay-metadataToHighlight.json,0.0027285099495202303,126,"4.1 Model Overview Figure 3 shows the overall architecture of Spotlighted Transcribing Network (STN), which consists of two main components: (1) a convolutional feature extractor network as the encoder, which learns the visual representations V from the input image x ; (2) a hierarchical transcribing decoder, which we mainly focus on in this work."
129,pagesOfTextToDisplay-metadataToHighlight.json,0.002679677214473486,129,4.2 Image Encoder The encoder part of STN is for extracting and embedding information from the image.
150,pagesOfTextToDisplay-metadataToHighlight.json,0.002612754702568054,150,W ′ W ′ W ′ . . .
67,pagesOfTextToDisplay-metadataToHighlight.json,0.002509430982172489,67,19 ] used an attention-based encoder-decoder system for character recognition problems.
116,pagesOfTextToDisplay-metadataToHighlight.json,0.002499704249203205,116,"in music score transcribing task, or a T E X token ( x , \frac , . . . )"
317,pagesOfTextToDisplay-metadataToHighlight.json,0.002397774253040552,317,We hope this work could lead to more studies in the future.
35,pagesOfTextToDisplay-metadataToHighlight.json,0.002197250258177519,35,Left is a music bar from Cello Suite No.
96,pagesOfTextToDisplay-metadataToHighlight.json,0.0020794046577066183,96,"Specifically, we compare our datasets with two commonly used datasets for scene text recognition, i.e., SVT [ 34 ] and IIIT5K [ 23 ], and conclude three main differences."
247,pagesOfTextToDisplay-metadataToHighlight.json,0.0018253392772749066,247,a) Melody Baseline Testing set percentage 40% 30% 20% 10% EncDec 0.266 0.272 0.277 0.282 AttnDot 0.524 0.548 0.580 0.617 AttnFC 0.683 0.710 0.730 0.756 AttnPos 0.725 0.736 0.741 0.758 STNM 0.729 0.733 0.749 0.759 STNR 0.738 0.748 0.758 0.767 (b) Formula Baseline Testing set percentage 40% 30% 20% 10% EncDec 0.405 0.427 0.445 0.451 AttnDot 0.530 0.563 0.600 0.611 AttnFC 0.657 0.701 0.717 0.725 AttnPos 0.716 0.723 0.732 0.741 STNM 0.717 0.726 0.740 0.749 STNR 0.739 0.751 0.759 0.778 (c) Multi-Line Baseline Testing set percentage 40% 30% 20% 10% EncDec 0.218 0.227 0.251 0.267 AttnDot 0.334 0.447 0.554 0.599 AttnFC 0.614 0.642 0.686 0.707 AttnPos 0.624 0.652 0.698 0.720 STNM 0.674 0.705 0.731 0.734 STNR 0.712 0.736 0.754 0.760 0 5 10 15 20 25 30 Epochs 0 .
267,pagesOfTextToDisplay-metadataToHighlight.json,0.0015879381680861115,267,"To conduct a fair comparison, the image encoders for baselines are changed to use the more recent ResNet [ 11 ] as our model does, with all of them tuned to have the best performance."
64,pagesOfTextToDisplay-metadataToHighlight.json,0.0015062770107761025,64,Xu et al. [
66,pagesOfTextToDisplay-metadataToHighlight.json,0.0015062770107761025,66,Lee et al. [
14,pagesOfTextToDisplay-metadataToHighlight.json,0.0014644971815869212,14,Abstracting with credit is permitted.
199,pagesOfTextToDisplay-metadataToHighlight.json,0.0014629478100687265,199,More detailed settings are presented in the experiment section.
235,pagesOfTextToDisplay-metadataToHighlight.json,0.0014556163223460317,235,"Besides, to prevent overfitting, we also add L2-regularization term in the loss function (Equation (5)), with the regularization amount adjusted to the best performance."
74,pagesOfTextToDisplay-metadataToHighlight.json,0.0013858150923624635,74,Table 1: The statistics of the datasets.
194,pagesOfTextToDisplay-metadataToHighlight.json,0.0012120804749429226,194,"Specifically, at time step t , with last spotlight history embedding denoted as e t , the current spotlight handle s t at time t is calculated as: s t = c ( e t ⊕ sc t − 1 ⊕ h t ; θ c ) (14) and the history embedding is updated by: e t = GRU ( s t − 1 , e t − 1 ; θ д ) (15) Through a separate module specifically for spotlight control, STN gains two advantages over the traditional attention mechanism."
282,pagesOfTextToDisplay-metadataToHighlight.json,0.0011940619442611933,282,"There are also similar observations as before, which demonstrates the effectiveness of STN framework again."
134,pagesOfTextToDisplay-metadataToHighlight.json,0.001068535028025508,134,"The output tensor also preserves spatial and contextual information, with the property that adjacent vectors representing neighboring parts of the image."
178,pagesOfTextToDisplay-metadataToHighlight.json,0.0010339762084186077,178,"By focusing directly on the correct spot, the transcription module therefore only cares about the local information, not confusing at areas with similar content all over the image."
107,pagesOfTextToDisplay-metadataToHighlight.json,0.0010216956725344062,107,"3.2 Problem Definition In this subsection, we formally introduce the structural image transcription problem."
63,pagesOfTextToDisplay-metadataToHighlight.json,0.0009931332897394896,63,3 ] proposed a way to jointly generate and align words using attention mechanism.
10,pagesOfTextToDisplay-metadataToHighlight.json,0.0009623985970392823,10,Transcribing Content from Structural Images with Spotlight Mechanism.
53,pagesOfTextToDisplay-metadataToHighlight.json,0.00094646803336218,53,"1 A domain specific language for music notation, http://lilypond.org/ 2 RELATED WORK The related research topics to our concerns can be classified into the following three categories: encoder-decoder system, attention mechanism, and reinforcement learning."
48,pagesOfTextToDisplay-metadataToHighlight.json,0.0009053904213942587,48,"Moreover, we propose two implementations on the basis of the STN framework."
283,pagesOfTextToDisplay-metadataToHighlight.json,0.0009030940709635615,283,"Clearly, from the results, both STNR and STNM converge faster than the other models, and also achieve a lower loss."
121,pagesOfTextToDisplay-metadataToHighlight.json,0.0008657702710479498,121,"Given a structural W × H image x , our goal is to transcribe the content from it as a sequence ˆ y = { ˆ y 1 , ˆ y 2 ,..., ˆ y T } as close as possible to the source code sequence y , where each ˆ y t is the predicted token taking from the specific language corresponding to the image."
59,pagesOfTextToDisplay-metadataToHighlight.json,0.0008600344299338758,59,"The whole architecture is end-to-end, which show the effectiveness in practice [30]."
176,pagesOfTextToDisplay-metadataToHighlight.json,0.0008164315368048847,176,"We also expand x t and y t as W ′ × H ′ matrices X t and Y t respectively, with same value for each element."
202,pagesOfTextToDisplay-metadataToHighlight.json,0.000774915621150285,202,"Second, the given token sequence may only be one of the many correct ones that all regenerates the original image."
159,pagesOfTextToDisplay-metadataToHighlight.json,0.0007615111535415053,159,y t y t y t . . .
154,pagesOfTextToDisplay-metadataToHighlight.json,0.0007593275513499975,154,X t  2 +  H ′ 1 2 . . .
89,pagesOfTextToDisplay-metadataToHighlight.json,0.0007399403839372098,89,"We exploit two real-world datasets, i.e., Melody and Formula , along with one synthetic dataset Multi-Line , specifically for the structural image transcription task 2 ."
195,pagesOfTextToDisplay-metadataToHighlight.json,0.000720311829354614,195,"First, STN focuses on local areas by design, and the model will only have to learn where to focus and what to transcribe, while the attention model have to first learn to focus, then learn what to focus on."
218,pagesOfTextToDisplay-metadataToHighlight.json,0.000716119073331356,218,"As the whole model is complicated, directly applying reinforcement learning to the model suffers from the large searching space."
36,pagesOfTextToDisplay-metadataToHighlight.json,0.0007142654503695667,36,1 in G major by Bach; Right is a function formula from a high school math exercise.
128,pagesOfTextToDisplay-metadataToHighlight.json,0.0007110315491445363,128,"In the following subsections, we will explain how each part of the STN works in detail."
84,pagesOfTextToDisplay-metadataToHighlight.json,0.0007049638661555946,84,"3.1 Data Description In this paper, we mainly focus on transcribing content from structural images."
302,pagesOfTextToDisplay-metadataToHighlight.json,0.0006926062051206827,302,"And in Figure 8, when Attn-Pos writes x , three x 's in the image all have high weights, causing the model to forget where to look next."
205,pagesOfTextToDisplay-metadataToHighlight.json,0.0006799031980335712,205,"Fortunately, in structural image transcription problems, we have an advantage that the process is reversible, meaning given the transcribed sequence, we can use a compiler to reconstruct Research Track Paper KDD 2018, August 19 ‒ 23, 2018, London, United Kingdom 2648 Transcribing Content from Structural Images with Spotlight Mechanism KDD '18, August 19–23, 2018, London, United Kingdom the image."
149,pagesOfTextToDisplay-metadataToHighlight.json,0.0006780928233638406,149,2 2 2 . . . . . . . . . . . . . . .
25,pagesOfTextToDisplay-metadataToHighlight.json,0.0006725225830450654,25,"In the literature, there are many efforts for this transcribing problem, especially on text reading task."
51,pagesOfTextToDisplay-metadataToHighlight.json,0.0006428909837268293,51,"We also design a reinforcement method to refine STN, self-improving the spotlight mechanism."
146,pagesOfTextToDisplay-metadataToHighlight.json,0.0006270990706980228,146,"The overall transcription loss L on an image-sequence pair is then defined as the negative log likelihood of the token sequence over the image: L = T Õ t = 1 − log P ( y t | y 1 ,..., y t − 1 , V ) . ("
137,pagesOfTextToDisplay-metadataToHighlight.json,0.0006267178687267005,137,"Hence, we can denote the probability of a decoder yielding a sequence y as: P ( y | x ) = T Ö t = 1 P ( y t | y 1 ,..., y t − 1 , V ) . ("
284,pagesOfTextToDisplay-metadataToHighlight.json,0.000618636782746762,284,"Especially, the improvements of them on the more complex Multi-Line datasets are more significant."
196,pagesOfTextToDisplay-metadataToHighlight.json,0.0006144989747554064,196,"Second, modeling reading and writing process as two separate sequences, with a standalone module dedicated for the ""where-to-look"" problem, STN is capable for directly learning a reading path on structural images apart from generating the output sequences, which enables our model to track the image structure more closely compared to attentive models where attentions and transcribing process are modeled together in only one network."
112,pagesOfTextToDisplay-metadataToHighlight.json,0.0005823401152156293,112,"Afterwards, the transcription module finds out ""what-to-write"" by utilizing the spotlighted information from the encoder, generating the transcribed content one token at a time."
285,pagesOfTextToDisplay-metadataToHighlight.json,0.0005673536215908825,285,"Thus, we can reach a conclusion that STN with spotlight mechanism has superior ability to transcribe content from structural images."
153,pagesOfTextToDisplay-metadataToHighlight.json,0.0005634174449369311,153,x t x t x t . . .
119,pagesOfTextToDisplay-metadataToHighlight.json,0.0005518502439372241,119,"Therefore, the problem can be defined as: Definition 3.1. ("
24,pagesOfTextToDisplay-metadataToHighlight.json,0.0005502221174538136,24,"As it is crucial in many applications, e.g., image retrieval [ 5 , 29 ], online education systems [ 13 , 20 ] and assistant devices [ 9 ], much attention has been attracted from both academia and industry [38]."
78,pagesOfTextToDisplay-metadataToHighlight.json,0.0005443256231956184,78,"For example, Ranzato et al. ["
62,pagesOfTextToDisplay-metadataToHighlight.json,0.0005443256231956184,62,"For example, Bahdanau et al. ["
155,pagesOfTextToDisplay-metadataToHighlight.json,0.0005420176894403994,155,H ′ 1 2 . . . . . . . . . . . . . . .
157,pagesOfTextToDisplay-metadataToHighlight.json,0.0005331448628567159,157,J − y t y t y t . . .
56,pagesOfTextToDisplay-metadataToHighlight.json,0.0005273743881843984,56,"Due to its remarkable performance, many efforts have been made to apply it to scene text recognition [ 35 ], aiming at transcribing texts from natural images."
98,pagesOfTextToDisplay-metadataToHighlight.json,0.0005248929955996573,98,"As shown in Table 1 and Figure 2, our datasets contain significantly longer content in relatively small images."
87,pagesOfTextToDisplay-metadataToHighlight.json,0.0005236237775534391,87,"Typical structural images include music scores, formulas and flow charts, etc.,"
158,pagesOfTextToDisplay-metadataToHighlight.json,0.0005139649147167802,158,y t y t y t . . . . . . . . . . . . . . .
151,pagesOfTextToDisplay-metadataToHighlight.json,0.000509290024638176,151,I − x t x t x t . . .
152,pagesOfTextToDisplay-metadataToHighlight.json,0.0005011726170778275,152,x t x t x t . . . . . . . . . . . . . . .
45,pagesOfTextToDisplay-metadataToHighlight.json,0.0004915398312732577,45,"To address the above challenges, following the observation on human transcribing process, i.e., first find out where to look, then write down the content, we present a two-stage ""where-to-what"" solution and propose a hierarchical framework called the S potlighted T ranscribing N etwork (STN) for transcribing content from structural images."
139,pagesOfTextToDisplay-metadataToHighlight.json,0.00048305006930604577,139,"Formally, at time step t , the hidden state for output history h t is updated based on the last output item y t − 1 and the previous output history h t − 1 , by an GRU network GRU (· ; θ h ) : h t = GRU ( y t − 1 , h t − 1 ; θ h ) . ("
190,pagesOfTextToDisplay-metadataToHighlight.json,0.0004737093113362789,190,"To track the image structure as a sequence with long-term dependency, we propose another GRU network (a) Markovian control module. ("
6,pagesOfTextToDisplay-metadataToHighlight.json,0.00047129913582466543,6,We also design a reinforcement method to refine our STN framework by self-improving the spotlight mechanism.
315,pagesOfTextToDisplay-metadataToHighlight.json,0.0004658779944293201,315,"Then, we applied supervised learning and reinforcement learning methods to accurately train and refine the spotlight modeling, in order to learn a reasonable reading path."
5,pagesOfTextToDisplay-metadataToHighlight.json,0.00046101343468762934,5,"Moreover, we propose two implementations on the basis of STN, i.e., STNM and STNR, where the spotlight movement follows the Markov property and Recurrent modeling, respectively."
133,pagesOfTextToDisplay-metadataToHighlight.json,0.0004590335302054882,133,"As a result, we construct an extractor network that takes an image x , outputs a 3 dimensional tensor V ( W ′ × H ′ × D ): V = f ( x ; θ f ) , (1) Research Track Paper KDD 2018, August 19 ‒ 23, 2018, London, United Kingdom 2646 Transcribing Content from Structural Images with Spotlight Mechanism KDD '18, August 19–23, 2018, London, United Kingdom where vector V ( i , j ) at each location ( i , j ) represents the local semantic information."
236,pagesOfTextToDisplay-metadataToHighlight.json,0.0004522406670730561,236,"At reinforcement stage, the discount factor γ is set as 0.99."
216,pagesOfTextToDisplay-metadataToHighlight.json,0.00045164013863541186,216,"The estimated value v t , i.e., the expected return, at time step t is then v t = v ( h t ⊕ sc t ⊕ s t ; θ v ) . ("
31,pagesOfTextToDisplay-metadataToHighlight.json,0.0004336619458626956,31,"In fact, there are many technical challenges along this line due to the unique characteristics of structural images."
179,pagesOfTextToDisplay-metadataToHighlight.json,0.0004330736992415041,179,"4.5 Spotlight Control Now we discuss how to control the spotlight to find a proper reading path, following the image structure through the whole generation process."
316,pagesOfTextToDisplay-metadataToHighlight.json,0.0004263823211658746,316,"Finally, we conducted extensive experiments on one synthetic and two real-world datasets to demonstrate the effectiveness of STN framework with fast model convergence and high performance, and also visualized the learned reading path."
211,pagesOfTextToDisplay-metadataToHighlight.json,0.00041521849925629795,211,"Reward: After taking the action, a reward signal r is received."
4,pagesOfTextToDisplay-metadataToHighlight.json,0.00041503316606394947,4,"Then, we decide ""what-to-write"" by developing a GRU based network with the spotlight areas for transcribing the content accordingly."
281,pagesOfTextToDisplay-metadataToHighlight.json,0.00040679145604372025,281,The losses of all models on the validation set throughout the training process on three datasets are shown in Figure 6.
307,pagesOfTextToDisplay-metadataToHighlight.json,0.0004046669346280396,307,There are still some directions for further studies.
115,pagesOfTextToDisplay-metadataToHighlight.json,0.0003944789059460163,115,"Each y t can be a LilyPond notation ( c , fis , . . . )"
99,pagesOfTextToDisplay-metadataToHighlight.json,0.0003934798005502671,99,"Sequences longer than 10 tokens taking 75.0%, 30.4% and 99.9% of Melody, Formula and Multi-Line datasets, respectively."
226,pagesOfTextToDisplay-metadataToHighlight.json,0.000391633715480566,226,"The images are randomly scaled and cropped for stable training, and ground-truth source code is cut into token sequences in the corresponding language to reduce searching space."
185,pagesOfTextToDisplay-metadataToHighlight.json,0.0003886472841259092,185,"Treating the spotlight handle as a Markov process means the probability of choosing s t at time t does not rely on spotlight handles more than one step earlier, i.e.: P ( s t | s 1 ,..., s t − 1 ; ·) = P ( s t | s t − 1 ; ·) . ("
122,pagesOfTextToDisplay-metadataToHighlight.json,0.000385001243557781,122,"4 SPOTLIGHTED TRANSCRIBING NETWORK In this section, we introduce the Spotlighted Transcribing Network (STN) framework in detail."
136,pagesOfTextToDisplay-metadataToHighlight.json,0.00037773852818645537,136,"4.3 Transcribing Decoder The transcribing decoder of STN, as in typical encoder-decoder architecture, generates one token at a time, by giving its conditional probability over the encoder output V and all the previous outputs { y 1 ,..., y t − 1 } at each time step t ."
277,pagesOfTextToDisplay-metadataToHighlight.json,0.0003740373067557812,277,"Third, STNR and STNM has slightly higher performance on Melody and Formula as Attn-Pos, but surpasses it marginally on Multi-Line dataset."
68,pagesOfTextToDisplay-metadataToHighlight.json,0.0003693847102113068,68,Our work improves the previous studies mainly from the following two aspects.
37,pagesOfTextToDisplay-metadataToHighlight.json,0.0003530343237798661,37,"placed simply from left to right, but the positions in the stave for each note are also specified, often with annotations added left or above."
217,pagesOfTextToDisplay-metadataToHighlight.json,0.0003450209042057395,217,"17) With a stochastic policy together with a value network, we can apply the actor-critic algorithm [ 2 ] to our sequence generation problem, with the policy network trained using policy gradient at each time step t as: ∇ θ = log π ( a | state t ; θ )( R t − v t ) , (18) and the value network trained by optimizing the distance between the estimated value and actual return: L v alue = || v t − R t || 2 2 ."
47,pagesOfTextToDisplay-metadataToHighlight.json,0.00034460320603102446,47,"Then, based on the learned spotlights areas, we aim for ""what-to-write"" problem and develop a GRU based network for transcribing the semantic content from the local spotlight areas."
288,pagesOfTextToDisplay-metadataToHighlight.json,0.00033988573704846203,288,"Research Track Paper KDD 2018, August 19 ‒ 23, 2018, London, United Kingdom 2650 Transcribing Content from Structural Images with Spotlight Mechanism KDD '18, August 19–23, 2018, London, United Kingdom Ground Truth: dis 16 a ' [ b ...fis] dis16 a[ b c b]... dis 16 b ' [ c STNR Attn-Pos high low high low Figure 7: Comparison between attention and spotlight mechanism on Melody dataset."
162,pagesOfTextToDisplay-metadataToHighlight.json,0.0003327548038214445,162,"4.4 Spotlight Mechanism In this subsection, we describe how to get focused information of the input image, i.e., the spotlight context sc t , with our proposed spotlight mechanism."
292,pagesOfTextToDisplay-metadataToHighlight.json,0.00033077766420319676,292,"Figure 7 and Figure 8 visualize the results throughout image examples from Melody and Formula datasets, respectively."
278,pagesOfTextToDisplay-metadataToHighlight.json,0.0003284513368271291,278,"These results demonstrate that STN with spotlight mechanism can well preserve the internal structure of images, especially in more complex scenarios, benefiting the transcription accuracy."
124,pagesOfTextToDisplay-metadataToHighlight.json,0.0003199721686542034,124,Then we describe all the details of our proposed spotlight mechanism in following sections.
186,pagesOfTextToDisplay-metadataToHighlight.json,0.00031936829327605665,186,"12) To decide where to put the spotlight properly, the model also needs to know current internal states at time step t , including the spotlight context sc t − 1 which represents previous spotlighted region, and the history embedding h t which represents output history before time t ."
289,pagesOfTextToDisplay-metadataToHighlight.json,0.00031764034065417945,289,\frac { \sqrt { x \frac { \sqrt { x f(x)= \frac{\sqrt{x- 1}}{x-2} Ground Truth: - } STNR Attn-Pos high low high low Figure 8: Comparison between attention and spotlight mechanism on Formula dataset.
297,pagesOfTextToDisplay-metadataToHighlight.json,0.00030801864340901375,297,"As shown in Figure 8, it fails to find the correct spot after generating "" \sqrt{x "", losing track of the radical expression, and generates the wrong token "" } "" at last. ("
106,pagesOfTextToDisplay-metadataToHighlight.json,0.00030474530649371445,106,"As a result, it is necessary to design a new approach that better fits this problem."
81,pagesOfTextToDisplay-metadataToHighlight.json,0.00030338874785229564,81,"3 PRELIMINARIES In this section, we first give a clear definition of structural images, and introduce the structural image datasets used in this paper."
130,pagesOfTextToDisplay-metadataToHighlight.json,0.00030283775413408875,130,"Instead of embedding the complete image x into one vector, which may cause a loss in structural information [ 36 ], we extract a set of feature vectors V , each of which is a D -dimensional representation corresponding to a part of the image: V = { V ( i , j ) : i = 1 ,..., W ′ , j = 1 ,..., H ′ } , V ( i , j ) ∈ R D ."
274,pagesOfTextToDisplay-metadataToHighlight.json,0.0003020773583557457,274,"This indicates that STN framework is more capable for structural image transcription tasks, being more effective and accurate on tracking complex image structures."
167,pagesOfTextToDisplay-metadataToHighlight.json,0.00029857587651349604,167,Inspired by Yang et al. [
88,pagesOfTextToDisplay-metadataToHighlight.json,0.0002981589059345424,88,"which can be described in music notation, T E X and UML code, respectively."
55,pagesOfTextToDisplay-metadataToHighlight.json,0.00029247597558423877,55,"Generally, the system has two separate parts, one encoder for representing and encoding the input information into a feature vector, and one decoder for generating the output sequence according to the encoded representation."
209,pagesOfTextToDisplay-metadataToHighlight.json,0.00029036510386504233,209,Action: Taking action a t is defined as generating the token y t at time step t .
161,pagesOfTextToDisplay-metadataToHighlight.json,0.0002900176332332194,161,"It should be clear that the element at each position ( i , j ) of the result matrix is [( i − x t ) 2 + ( j − y t ) 2 ]/ σ 2 t ."
175,pagesOfTextToDisplay-metadataToHighlight.json,0.000289604882709682,175,"Specifically, as shown in Figure 4, for each point ( i , j ) , we have I ( i , j ) = i and J ( i , j ) = j ."
169,pagesOfTextToDisplay-metadataToHighlight.json,0.00028668323648162186,169,"Formally, under the spotlight with handle s t = ( x t , y t , σ t ) T , the weights for each vector at position ( i , j ) at time step t , denoted as α ( i , j ) t , is proportional to the probability density at point ( i , j ) under Gaussian distribution: α ( i , j ) t ∼N(( i , j ) T | μ t , Σ t ) , (6) μ t = ( x t , y t ) T Σ t =  σ t 0 0 σ t  . ("
170,pagesOfTextToDisplay-metadataToHighlight.json,0.00028550776187330484,170,"7) Intuitively, the closer ( i , j ) is to the center ( x t , y t ) , the higher the weight should be, mimicking shedding a spotlight with radius σ t onto the location ( x t , y t ) ."
123,pagesOfTextToDisplay-metadataToHighlight.json,0.00028421820024959743,123,First we give an overview of the model architecture.
214,pagesOfTextToDisplay-metadataToHighlight.json,0.00028306894819252193,214,"The goal is to maximize the sum of the discounted rewards from each time t , i.e., the return: R t = T Õ k = t γ k r k . ("
32,pagesOfTextToDisplay-metadataToHighlight.json,0.00028282159473747015,32,"First, different from natural images, where the text content is mostly placed in simple patterns, in structural images, the content objects usually follow a fine-grained grammar, and are organized in a more complex manner."
265,pagesOfTextToDisplay-metadataToHighlight.json,0.0002821856178343296,265,0 Loss Enc-Dec Attn-Dot Attn-FC Attn-Pos STNM STNR (c) Multi-Line Figure 6: Validation loss of all models on three datasets. •
165,pagesOfTextToDisplay-metadataToHighlight.json,0.00027885238523595035,165,"To achieve this goal, we propose a novel spotlight mechanism to mimic human focus directly, where at each time step, we only care about information around a certain location which we call a spotlight center, by ""shedding"" a spotlight around it."
221,pagesOfTextToDisplay-metadataToHighlight.json,0.0002766409597825259,221,"With this train-and-refine procedure, our model can learn a reasonable reading path on structural images, focusing on different parts following the image structure when transcribing, and get superior transcription results, as our experimental results show in the next section."
95,pagesOfTextToDisplay-metadataToHighlight.json,0.0002650801616255194,95,We now conduct deep analysis to show the unique characteristics of the structural image transcription task compared to traditional scene text recognition.
204,pagesOfTextToDisplay-metadataToHighlight.json,0.0002647039364092052,204,Fitting to only one of the correct sequences lets down the model even when it achieves good strategies.
80,pagesOfTextToDisplay-metadataToHighlight.json,0.00026374042499810457,80,"2 ] utilized the actor-critic algorithm for sequence prediction, refining the model to improve sentence BLEU score."
42,pagesOfTextToDisplay-metadataToHighlight.json,0.0002608700015116483,42,"Thus, it is very challenging to transcribe the complete content from an area containing such a informative object, compared to just one character in tasks such as scene text recognition."
127,pagesOfTextToDisplay-metadataToHighlight.json,0.0002541364810895175,127,"Mimicking human reading process, the decoder first takes the encoded image information V and find out ""where-to-look"" by shedding spotlight on it, following the learned reading path, then generates the token sequence y , by predicting one token at a time using a GRU-based output network, solving the ""what-to-write"" problem."
131,pagesOfTextToDisplay-metadataToHighlight.json,0.000251064047915861,131,"A deep convolutional neural network (CNN) is used as the feature extractor to capture high-level semantic information, which we denote as f (· ; θ f ) ."
50,pagesOfTextToDisplay-metadataToHighlight.json,0.00024909008061513305,50,"Comparatively, the second is a more sophisticated one, i.e., STNR with Recurrent modeling , which can track long-term characteristics of spotlight movements."
23,pagesOfTextToDisplay-metadataToHighlight.json,0.0002489916223566979,23,"It is an essential problem for computers to understand how humans communicate about what they see, which includes many tasks, such as reading text from scenes [ 17 , 40 ], writing notes from music scores [ 28 ] and recognizing formulas from pictures [6]."
201,pagesOfTextToDisplay-metadataToHighlight.json,0.00024511790252290666,201,"Firstly, the model has to jointly learn two different sequences with only one of them directly supervised, which may result in inaccurate reading path."
203,pagesOfTextToDisplay-metadataToHighlight.json,0.00024284345272462815,203,"For instance, in LilyPond notation, we can optionally omit duration for notes at same length with their predecessors."
220,pagesOfTextToDisplay-metadataToHighlight.json,0.000225392883294262,220,"Therefore, at reinforcement stage, we only optimize parameters from the spotlight control module ( θ n in STNM, θ c and θ д in STNR), along with those from the output layer ( θ o ), and omit θ f and θ h , which reduces the variance when applying reinforcement learning algorithms, and get better improvements."
143,pagesOfTextToDisplay-metadataToHighlight.json,0.0002194352709921077,143,"We refer to the spotlight center position as s t at time step t , and the spotlighted information as spotlight context sc t ."
234,pagesOfTextToDisplay-metadataToHighlight.json,0.00021270685829222202,234,"Each parameter is sampled from U  − p 6 /( n in + n out ) , p 6 /( n in + n out )  as their initial values, where n in , n out stands for the number of neurons feeding in and neurons the result is fed to, respectively."
104,pagesOfTextToDisplay-metadataToHighlight.json,0.00021013952209614217,104,"Third, structural image transcription process is reversible, meaning the corresponding code should be able to compile and regenerate the original image, which is not necessary or possible for traditional scene text recognition."
200,pagesOfTextToDisplay-metadataToHighlight.json,0.00020817569748032838,200,"Though our model is differentiable, and can be optimized through back-propagation methods, directly training to fit the label suffers from some specific aspects in the image transcribing task."
215,pagesOfTextToDisplay-metadataToHighlight.json,0.0002045155706582591,215,"16) We further define a value network v (· ; θ v ) for estimation of the expected return from each state t , which is a feed-forward network with the same input as the STN output layer d ."
97,pagesOfTextToDisplay-metadataToHighlight.json,0.0002013684861594811,97,"First, structural image transcription needs to preserve more information: other than just objects, how they are organized should also be transcribed."
75,pagesOfTextToDisplay-metadataToHighlight.json,0.0001999895612243563,75,Dataset Image count Token space Token count Avg.
310,pagesOfTextToDisplay-metadataToHighlight.json,0.0001921220391523093,310,"Second, we will try to apply our model to some more ambitious settings, such as transcribing with long-term context, also to make our model capable for other transcribing applications such as scene text recognition."
39,pagesOfTextToDisplay-metadataToHighlight.json,0.00019181935931555927,39,"Thus, it is necessary for transcribing to not only capture the information from local areas, but also preserve the internal structure and organization of the content."
208,pagesOfTextToDisplay-metadataToHighlight.json,0.00019105945830233395,208,"Therefore, instead of directly using the environment state, we use the internal states (combined and denoted as state t ) in STN framework as MDP states."
224,pagesOfTextToDisplay-metadataToHighlight.json,0.0001908353588078171,224,"We partition all our datasets, i.e., Melody , Formula and Multi-Line , into 60%/40%, 70%/30%, 80%/20%, 90%/10% as training/testing sets, respectively, to test model performance at different data sparsity."
172,pagesOfTextToDisplay-metadataToHighlight.json,0.00018857078975997865,172,"To parallize the calculation of Equation (9), we perform a small trick as demonstrated in Figure 4."
103,pagesOfTextToDisplay-metadataToHighlight.json,0.00018680647190194577,103,"Hence, it is even more complicated to transcribe content from structural images compared to text recognition."
314,pagesOfTextToDisplay-metadataToHighlight.json,0.00018173565331380814,314,"Specifically, we first designed a two-stage ""where-to-what"" solution with a novel spotlight mechanism dedicated for the ""whereto-look"" problem, providing two implementations under the framework, modeling the spotlight movement through Markov chain and recurrent dependency, respectively."
239,pagesOfTextToDisplay-metadataToHighlight.json,0.00018103299953509122,239,"To demonstrate the effectiveness of STN, we compare our two implementations, i.e., STNM and STNR, with many state-of-the-art baselines as follows. •"
94,pagesOfTextToDisplay-metadataToHighlight.json,0.00017641503654886037,94,We summarize some basic statistics of these datasets in Table 1.
271,pagesOfTextToDisplay-metadataToHighlight.json,0.0001725784968584776,271,We repeat all experiments 5 times and report the average results which are shown in Table 2.
181,pagesOfTextToDisplay-metadataToHighlight.json,0.00017073217895813286,181,"We provide two implementations under the STN framework, i.e., the straightforward STNM with Markov property , and the more sophisticated STNR with Recurrent modeling , utilizing another GRU network."
230,pagesOfTextToDisplay-metadataToHighlight.json,0.00016916079039219767,230,"For its transcribing decoder, we set the output history embedding h t , and the spotlight history embedding e t as the same dimensions of 128, respectively."
305,pagesOfTextToDisplay-metadataToHighlight.json,0.00016346029588021338,305,All the above experiments have shown the effectiveness of STN on structural image transcription tasks.
43,pagesOfTextToDisplay-metadataToHighlight.json,0.00015557208098471165,43,"Third, there exist plenty of similar objects puzzling the transcribing task, e.g., a sixteenth note (blue in Figure 1(a)) just contains one more flag on the stem than an eighth note (red), while notes with same duration and different pitches are almost identical except for their positioning."
291,pagesOfTextToDisplay-metadataToHighlight.json,0.00015362842532340437,291,"To show the effectiveness of STN capturing the image structure and producing a reasonable reading path while transcribing, we visualize the spotlight weights computed by STNR when generating tokens, and compare them with the attention weights calculated by Attn-Pos model."
192,pagesOfTextToDisplay-metadataToHighlight.json,0.00015251517470460385,192,t - 1 s t - 1 s t s t s t s t + 1 s t h t h t + 1 h Figure 5: The spotlight control module implementations.
38,pagesOfTextToDisplay-metadataToHighlight.json,0.00015173446445260197,38,"A division formula in Figure 1(b) contains nested structure, where the equation components are placed at the left and right side of the equal sign, with two parts of the right-hand-side fraction placed above and below the middle line."
44,pagesOfTextToDisplay-metadataToHighlight.json,0.00014972065400797874,44,This characteristic requires a careful design for the transcribing.
102,pagesOfTextToDisplay-metadataToHighlight.json,0.00014786732208449394,102,"Second, the output space and count in our datasets are often larger than SVT and IIIT5K, as shown in Table 1."
11,pagesOfTextToDisplay-metadataToHighlight.json,0.00013627888984046876,11,In KDD '18: The 24th ACM SIGKDD International ∗ The corresponding author.
105,pagesOfTextToDisplay-metadataToHighlight.json,0.00013620505342260003,105,"In summary, the above analysis clearly shows that the structural image transcription problem is quite different from traditional scene text recognition tasks."
198,pagesOfTextToDisplay-metadataToHighlight.json,0.00013537186896428466,198,"The parameters are updated to minimize the total transcription loss L (Equation (5)) through a gradient descent algorithm, which we choose the Adam optimizer [ 18 ]."
295,pagesOfTextToDisplay-metadataToHighlight.json,0.00013260003470350057,295,"In the melody example, it focuses on notes from left to right, and also tracks the height of each note, making accurate note pitch prediction; In the formula example, it clearly follows middle-top-bottom order when reading a fraction."
293,pagesOfTextToDisplay-metadataToHighlight.json,0.0001298314455198124,293,"5 In each example, we compare the attention and spotlight mechanism on how focused they are when generating a token, also on how well they track the image structure."
241,pagesOfTextToDisplay-metadataToHighlight.json,0.0001285546168219298,241,Its design allows it to be used in our problem setup with minor adjustments. •
298,pagesOfTextToDisplay-metadataToHighlight.json,0.0001268149062525481,298,"2) Although Attn-Pos model assigns more weights on content objects in images, e.g., notes, formulas and variables, it is often confused at areas with similar content."
242,pagesOfTextToDisplay-metadataToHighlight.json,0.00012269942089915276,242,"Attn-Dot is an encoder-decoder model with attention mechanism following [ 22 ], where the attention score is calculated by directly computing the similarity between current output state and each encoded image vectors. •"
57,pagesOfTextToDisplay-metadataToHighlight.json,0.00011428693687776104,57,"Specifically, for encoder design, representative works leveraged deep CNN based networks, which have been the most popular methods due to their performance on hierarchical feature extraction [ 26 ], to learn the information encodings from images [ 16 ]."
65,pagesOfTextToDisplay-metadataToHighlight.json,0.00011194602848263457,65,36 ] proposed soft and hard attention mechanisms for image captioning.
187,pagesOfTextToDisplay-metadataToHighlight.json,0.00011163133603986353,187,"Thus, we can use a feed-forward neural network n (· ; θ n ) to model the choice of s t (Figure 5 (a)) as: s t = n ( s t − 1 ⊕ sc t − 1 ⊕ h t ; θ n ) (13) The way we model the sequence is simple and time-independent, which makes it easier for the controlling module to train."
125,pagesOfTextToDisplay-metadataToHighlight.json,0.00011038794036721811,125,Finally we discuss the training process of STN with reinforcement learning for refinement.
279,pagesOfTextToDisplay-metadataToHighlight.json,0.00010875333100557327,279,"Last but not least, we can see that STNR consistently outperforms than STNM, which indicates that it is effective to track long-term dependency for spotlighting in the process of transcribing structural image content."
135,pagesOfTextToDisplay-metadataToHighlight.json,0.0001070804355549626,135,This allows the decoder module to use the image information selectively with both content and location in mind.
92,pagesOfTextToDisplay-metadataToHighlight.json,0.00010302969167241827,92,"The Formula dataset is collected from Zhixue.com, an online educational system, which contains 61649 printed formulas from high school math exercises, with their corresponding T E X code."
193,pagesOfTextToDisplay-metadataToHighlight.json,0.00010081382788484916,193,"GRU (· ; θ д ) to track the spotlight history, and a fully connected layer c (· ; θ c ) to generate next spotlight handle (Figure 5 (b))."
313,pagesOfTextToDisplay-metadataToHighlight.json,0.00010025446681538597,313,images by finding a reading path tracking the image internal structure.
228,pagesOfTextToDisplay-metadataToHighlight.json,9.99404292088002e-05,228,"We now specify the model setup in STN, including image encoder, transcription decoder and reinforcement module."
100,pagesOfTextToDisplay-metadataToHighlight.json,9.614878217689693e-05,100,"However, only 1.9% in SVT and 2.7% in IIIT5K have more than 10 character long sequences."
212,pagesOfTextToDisplay-metadataToHighlight.json,9.541813778923824e-05,212,"Here we define the reward r t as 0 when the generation is not finished at time step t , or the pixel similarity between the reconstruction image and the original image after the whole generation process finished."
164,pagesOfTextToDisplay-metadataToHighlight.json,9.240261715603992e-05,164,"As mentioned earlier, the visual embedding V is confounding for the decoder, and we want to focus on one spot at a time when generating output."
15,pagesOfTextToDisplay-metadataToHighlight.json,9.145354124484584e-05,15,"To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee."
17,pagesOfTextToDisplay-metadataToHighlight.json,9.11581955733709e-05,17,"KDD '18, August 19–23, 2018, London, United Kingdom © 2018 Association for Computing Machinery."
160,pagesOfTextToDisplay-metadataToHighlight.json,9.096370922634378e-05,160,Y t  2 # / σ 2 t Figure 4: Demonstration of the parallelized operation on assigning weights.
231,pagesOfTextToDisplay-metadataToHighlight.json,9.053645771928132e-05,231,"The value network used at the reinforcement stage is a two-layer fullyconnected neural network, with the hidden layer also sized at 128."
71,pagesOfTextToDisplay-metadataToHighlight.json,8.871835598256439e-05,71,"Second, previous decoding process has one RNN for learning attentions and transcribing simultaneously, which may cause some confusion for transcription, while our framework models spotlighting and transcribing with two separate facilities, avoiding the confusion between two sequences."
60,pagesOfTextToDisplay-metadataToHighlight.json,8.860932575771585e-05,60,"2.2 Attention Mechanism However, in the original encoder-decoder systems, encoding the whole input into one vector usually makes the encoded information of images clumsy and confusing for the decoder to read from, leading to unsatisfactory transcription [ 22 ]."
163,pagesOfTextToDisplay-metadataToHighlight.json,8.839922520564869e-05,163,"How the spotlight moves through time is handled in a separate spotlight control module, and is described later in detail in Section 4.5."
91,pagesOfTextToDisplay-metadataToHighlight.json,8.742546197026968e-05,91,"3 http://web.mit.edu/music21/ written by Bach, split into 1 to 4 bar length, forming 4208 imagecode pairs."
244,pagesOfTextToDisplay-metadataToHighlight.json,8.577468543080613e-05,244,"The model presents two attention strategies, i.e., the ""hard"" and ""soft"" attention mechanism, from which we follow [ 36 ] and choose the more widely used ""soft"" attention as it is deterministic and easier to train."
147,pagesOfTextToDisplay-metadataToHighlight.json,8.461393736070022e-05,147,"5) With all the calculation being deterministic and differentiable, the model can be optimized through standard back-propagation. """
270,pagesOfTextToDisplay-metadataToHighlight.json,8.380777580896392e-05,270,"We train STN along with all the baseline models on four different data partition of each, comparing token accuracy at different data sparsity."
275,pagesOfTextToDisplay-metadataToHighlight.json,8.200340380426496e-05,275,"Second, STN models, as well as attention based methods, all have much higher prediction accuracy than plain EncDec method, which proves the claim mentioned earlier in this paper that image information encoded as a single vector is confounding for decoder to decode, and both STN and attentive 4 http://pytorch.org models are able to reduce the confusion."
3,pagesOfTextToDisplay-metadataToHighlight.json,8.013073238544166e-05,3,"Specifically, we first decide ""where-tolook"" through a novel spotlight mechanism to focus on different areas of the original image following its structure."
276,pagesOfTextToDisplay-metadataToHighlight.json,7.974451000336558e-05,276,"Moreover, STN models are consistently better than those attentive ones, showing the superiority of STN with separate modules for spotlighting and transcribing."
79,pagesOfTextToDisplay-metadataToHighlight.json,7.903717050794512e-05,79,"27 ] used the whole sequence metrics to guide the sequence generation, using REINFORCE method; Bahdanau et al. ["
140,pagesOfTextToDisplay-metadataToHighlight.json,7.393550185952336e-05,140,"3) For image part, the visual representation V we get as the encoder output carries enough semantic information, but as a whole it can be confounding for the decoder to comprehend, and thus needs careful selection [ 36 ]."
294,pagesOfTextToDisplay-metadataToHighlight.json,7.306670158868656e-05,294,"From the visualization, we can draw conclusions that: (1) STNR finds a more reasonable reading path on both examples."
145,pagesOfTextToDisplay-metadataToHighlight.json,7.280516729224473e-05,145,"With embedded history h t , and spotlight context sc t , together with current spotlight position s t , the conditional probability of output token at time t can then be parameterized as follows: P ( y t | y 1 ,..., y t − 1 , V ) = Softmax ( d ( h t ⊕ sc t ⊕ s t ; θ d )) , (4) where d (· ; θ d ) is a transformation function (e.g. a feed-forward neural network) that outputs a vocabulary-sized vector, and ⊕ represents the operation that concatenates two vectors."
70,pagesOfTextToDisplay-metadataToHighlight.json,7.220316911116242e-05,70,"In our work, we propose a novel spotlight mechanism to directly find a reading path tracking the image structure for transcribing."
299,pagesOfTextToDisplay-metadataToHighlight.json,7.217377424240112e-05,299,"On the other hand, STNR clearly distinguishes similar regions properly."
166,pagesOfTextToDisplay-metadataToHighlight.json,7.167465810198337e-05,166,"More specifically, we define a spotlight handle s t = ( x t , y t , σ t ) T at each time step t to represent the spotlight, where ( x t , y t ) represents the center position of the spotlight, and σ t represents the radius of the spotlight."
184,pagesOfTextToDisplay-metadataToHighlight.json,7.160922541515902e-05,184,"With an assumption that is not far from reality, we can intuitively treat the spotlight handle sequence as a Markov process, i.e., current spotlight handle only depends on the previous handle, along with other internal states at current time step."
109,pagesOfTextToDisplay-metadataToHighlight.json,7.010234548943117e-05,109,"Each Research Track Paper KDD 2018, August 19 ‒ 23, 2018, London, United Kingdom 2645 KDD '18, August 19–23, 2018, London, United Kingdom Y. Yin et al."
245,pagesOfTextToDisplay-metadataToHighlight.json,6.995469448156655e-05,245,"Research Track Paper KDD 2018, August 19 ‒ 23, 2018, London, United Kingdom 2649 KDD '18, August 19–23, 2018, London, United Kingdom Y. Yin et al."
243,pagesOfTextToDisplay-metadataToHighlight.json,6.966257933527231e-05,243,"Attn-FC is an encoder-decoder model similar to [ 33 ], but with basic visual attention strategy."
171,pagesOfTextToDisplay-metadataToHighlight.json,6.956302240723744e-05,171,"To calculate the weight α ( i , j ) t of each position ( i , j ) while still make the process differentiable, we apply the definition of Gaussian distribution and rewrite the expression of α ( i , j ) t as: α ( i , j ) t = Softmax ( b t ) = exp ( b ( i , j ) t ) Í W ′ u = 1 Í H ′ v = 1 exp ( b ( u , v ) t ) , (8) b ( i , j ) t = − ( i − x t ) 2 + ( j − y t ) 2 σ 2 t , (9) where b measures how close the point ( i , j ) is to the center ( x t , y t ) , i.e., how important this point is, and α is thus a W ′ × H ′ matrix following the truncated Gaussian distribution for each point ( i , j ) , and can later be used as weights for each image feature vector."
300,pagesOfTextToDisplay-metadataToHighlight.json,6.909287913003936e-05,300,"More specifically, in Figure 7, although Attn-Pos is able to focus on the notes, all notes are given similar weights as they look similar, which causes confusion and then 5 We only choose two real-world datasets for visualization due to the page limitation."
207,pagesOfTextToDisplay-metadataToHighlight.json,6.890382064739242e-05,207,"Formally, we define the state , action and reward of the MDP as follows: State: View our problem as outputting the probability of items at each time step conditioned by the image and previous generations, the environment state at time step t as the combination of the image x and the output history { y 1 ,..., y t − 1 } , which is exactly the inputs of the STN."
33,pagesOfTextToDisplay-metadataToHighlight.json,6.857734842924401e-05,33,"E.g., in Figure 1(a), notes from the music score are not only Research Track Paper KDD 2018, August 19 ‒ 23, 2018, London, United Kingdom 2643 KDD '18, August 19–23, 2018, London, United Kingdom Y. Yin et al."
173,pagesOfTextToDisplay-metadataToHighlight.json,6.854650564491749e-05,173,"We first construct two W ′ × H ′ matrices I and J in advance, each of them representing one Research Track Paper KDD 2018, August 19 ‒ 23, 2018, London, United Kingdom 2647 KDD '18, August 19–23, 2018, London, United Kingdom Y. Yin et al."
312,pagesOfTextToDisplay-metadataToHighlight.json,6.850358477095142e-05,312,"6 CONCLUSION In this paper, we presented a novel hierarchical Spotlighted Transcribing Network (STN) for transcribing content from structural Research Track Paper KDD 2018, August 19 ‒ 23, 2018, London, United Kingdom 2651 KDD '18, August 19–23, 2018, London, United Kingdom Y. Yin et al."
29,pagesOfTextToDisplay-metadataToHighlight.json,6.821894930908456e-05,29,"Though good performances have been achieved, previous studies mainly focus on the images with straightforward content (i.e., text with characters), while ignoring large proportion of structural images, where the content objects are well-formed in complex manners, e.g., music scores (Figure 1(a)) and formulas (Figure 1(b))."
141,pagesOfTextToDisplay-metadataToHighlight.json,6.816308450652286e-05,141,"To deal with this problem, we mimic what human do when reading images: focus on one spot at a time, write down content, then focus on a next spot following the image structure [ 4 ]."
22,pagesOfTextToDisplay-metadataToHighlight.json,6.773693894501776e-05,22,"org/10.1145/3219819.3219962 1 INTRODUCTION Transcribing content from images refers to recognizing semantic information in images into comprehensible forms (e.g., text) in computer vision [ 38 ]."
180,pagesOfTextToDisplay-metadataToHighlight.json,6.761926488252357e-05,180,"Different from traditional attention strategy where both output sequence and attention behavior are embedded in one module, we see the spotlight movement (i.e., the value of the spotlight handle s t = ( x t , y t , σ t ) T at each time step t ) as a separate sequence devoted to following the image structure, and model this sequence with a standalone spotlight controlling module, without mixing the information with the output sequence."
69,pagesOfTextToDisplay-metadataToHighlight.json,6.612133438466117e-05,69,"First, the attention weights are usually calculated by the correspondence between outputs and the whole content, which let the models know ""what"" to look but not ""where"" to look."
237,pagesOfTextToDisplay-metadataToHighlight.json,6.550818943651393e-05,237,"We also apply some techniques mostly mentioned in [ 2 ] to reduce variance, including using an additional target Q-network and reward normalization."
54,pagesOfTextToDisplay-metadataToHighlight.json,6.517466681543738e-05,54,"2.1 Encoder-Decoder System The encoder-decoder system is a general framework, which has been applied to many applications, such as neural machine translation [ 3 , 7 ] and image captioning [ 33 , 36 ]."
61,pagesOfTextToDisplay-metadataToHighlight.json,6.504423072328791e-05,61,"To improve the encoderdecoder models addressing this problem, inspired by human visual system, researchers have tried to propose many attention mechanisms to highlight different parts of the encoder output by assigning weights to encoding vectors in each step of text generation [ 3 , 24 , 36 ] or sequential prediction [ 31 , 39 ]."
46,pagesOfTextToDisplay-metadataToHighlight.json,6.392805516952649e-05,46,"Specifically, after encoding images as features vectors, in our decoder component, we first propose a spotlight module with a novel mechanism to handle the ""where-to-look"" problem and decide a reading path focusing on areas of the original image following its internal structure."
110,pagesOfTextToDisplay-metadataToHighlight.json,6.344197754515335e-05,110,"Transcribing Decoder Transcription Module h t-1 t - 1 control control CNN (dis) (16) (a) V Image Encoder Spotlight Module control y t y t + 1 y t - 1 h t + 1 h t h t - 1 s t s t + 1 s t - 1 sc t sc t + 1 sc Figure 3: The STN model architecture consists of two main parts: 1) a convolutional image feature extractor as the encoder, and 2) the transcribing decoder."
303,pagesOfTextToDisplay-metadataToHighlight.json,6.128823588369414e-05,303,"On the contrary, STNR is well focused on the correct spot when generating each token on both of the datasets, which leads to more precise predictions."
177,pagesOfTextToDisplay-metadataToHighlight.json,6.0053967899875715e-05,177,"Therefore, Equation (9) can be written as the matrix form: b t = −[( I − X t ) 2 + ( J − Y t ) 2 ]/ σ 2 t (10) The focused information of the visual representation V at time step t can then be computed as a spotlight context vector sc t weighted by α ( i , j ) t according to current spotlight handle s t , i.e., the weighted sum of features at each position: sc t = W ′ Õ i = 1 H ′ Õ j = 1 α ( i , j ) t V ( i , j ) (11) Please note that the spotlight context sc t represents the information in the focused area at time step t , and should contain useful information specifically for transcribing at current time step."
52,pagesOfTextToDisplay-metadataToHighlight.json,5.8999357861466706e-05,52,"We conduct extensive experiments on real-world structural image datasets, where the results clearly demonstrate the effectiveness of the STN framework."
13,pagesOfTextToDisplay-metadataToHighlight.json,5.882136247237213e-05,13,Copyrights for components of this work owned by others than ACM must be honored.
26,pagesOfTextToDisplay-metadataToHighlight.json,5.867080108146183e-05,26,"Among them, the most representative one called Optical Character Recognition (OCR) has been extensively studied in many decades [ 14 ], which mainly follows rule-based solutions for generating texts from well-scanned documents [ 21 ]."
296,pagesOfTextToDisplay-metadataToHighlight.json,5.778002378065139e-05,296,"AttnPos model on the other hand, does not track the image structure well enough."
101,pagesOfTextToDisplay-metadataToHighlight.json,5.739537300541997e-05,101,"In addition, Melody, Formula and Multi-Line contain in average 1.26, 8.15 and 4.14 tokens every 1000 pixels, while SVT and IIIT5k only contain 0.46 and 0.43 characters, respectively, which indicates that each proportion of an image contains more information to be transcribed, along with the informative structure."
144,pagesOfTextToDisplay-metadataToHighlight.json,5.726583913201466e-05,144,"Further details on how to get focused spotlight context are described in Section 4.4, while how to move the spotlight following the structure is described in Section 4.5."
168,pagesOfTextToDisplay-metadataToHighlight.json,5.6879980547819287e-05,168,"37 ], we ""shed"" a spotlight by assigning weights to image representation vectors at each position, following a truncated Gaussian distribution centered at ( x t , y t ) , with the same variance σ t on both axis."
142,pagesOfTextToDisplay-metadataToHighlight.json,5.6295473768841475e-05,142,"Along this line, we propose a module with novel spotlight mechanism, where at each time step, we only focus on information around a certain spotlight center."
7,pagesOfTextToDisplay-metadataToHighlight.json,5.558906195801683e-05,7,"We conduct extensive experiments on many structural image datasets, where the results clearly demonstrate the effectiveness of STN framework."
229,pagesOfTextToDisplay-metadataToHighlight.json,5.534286901820451e-05,229,"For STN image encoder, we use a variation of ResNet [ 11 ], and set the encoded vector width as 128."
73,pagesOfTextToDisplay-metadataToHighlight.json,5.3481824579648674e-05,73,"The main idea of them is to learn and refine model Research Track Paper KDD 2018, August 19 ‒ 23, 2018, London, United Kingdom 2644 Transcribing Content from Structural Images with Spotlight Mechanism KDD '18, August 19–23, 2018, London, United Kingdom 0 20 40 Content length 0 2 4 6 8 Percentage (%) (a) Melody 0 20 40 Content length 0 5 10 15 20 Percentage (%) (b) Formula 0 50 100 Content length 0 1 2 3 4 Percentage (%) (c) Multi-Line 0 20 40 Content length 0 5 10 15 20 Percentage (%) (d) SVT 0 20 40 Content length 0 5 10 15 Percentage (%) (e) IIIT5K Figure 2: Comparison of structural image (blue) and scene text recognition datasets (red) on content length distribution."
72,pagesOfTextToDisplay-metadataToHighlight.json,5.1747061661444604e-05,72,"2.3 Reinforcement Learning Deep reinforcement learning is a kind of state-of-the-art technique, which has shown superior abilities in many fields, such as gaming and robotics [ 1 ]."
114,pagesOfTextToDisplay-metadataToHighlight.json,5.147963383933529e-05,114,"For each image, the expected output, i.e., its source code, is given as a token sequence y = { y 1 , y 2 ,..., y T } , where T is the length of token sequence."
222,pagesOfTextToDisplay-metadataToHighlight.json,5.1444418204482645e-05,222,"5 EXPERIMENTS In this section, we conduct extensive experiments to demonstrate the effectiveness of STN model from various aspects: (1) the transcribing performance; (2) the validation loss demonstrating the model sensitivity; (3) the spotlight visualization of STN."
311,pagesOfTextToDisplay-metadataToHighlight.json,5.125307143316604e-05,311,"Third, we would like to further decouple the reading and writing process of STN, in order to mimic human behavior more genuinely."
19,pagesOfTextToDisplay-metadataToHighlight.json,5.024870188208297e-05,19,"$15.00 https://doi.org/10.1145/3219819.3219962 Conference on Knowledge Discovery & Data Mining, August 19–23, 2018, London, United Kingdom."
197,pagesOfTextToDisplay-metadataToHighlight.json,5.0235146773047745e-05,197,"4.6 Training and Refining STN Parameters to be updated in both implementations comes from three parts: the encoder parameters θ f , the decoder parameters { θ h , θ d } , and parameters in the spotlight control module, which are θ n in STNM and { θ c , θ д } in STNR."
240,pagesOfTextToDisplay-metadataToHighlight.json,4.778198490384966e-05,240,Enc-Dec is a plain encoder-decoder model used originally for image captioning [ 33 ].
27,pagesOfTextToDisplay-metadataToHighlight.json,4.7239776904461905e-05,27,"Recently, researchers focus on a more general scene text recognition task, aiming to recognize texts from natural images [ 33 ]."
219,pagesOfTextToDisplay-metadataToHighlight.json,4.718809941550717e-05,219,"Through experiments we notice that, after supervised training, the image extractor and the output history embedding modules have both been trained properly, and it is more important for our framework to have a better reading path to make precise predictions, which indicates that refining the spotlight module is most beneficial."
113,pagesOfTextToDisplay-metadataToHighlight.json,4.7053654270712286e-05,113,"input image x is a one-channel gray-scale image with width W and height H , containing content such as music notations or printed formulas."
108,pagesOfTextToDisplay-metadataToHighlight.json,4.676388925872743e-05,108,"In our image transcribing applications, we are given structural images and their corresponding source code."
206,pagesOfTextToDisplay-metadataToHighlight.json,4.565404378809035e-05,206,"With the guidance of this, we can further refine our model using reinforcement learning, by regarding our sequential generation as a decision making problem, viewing it as a Markov Decision Process (MDP) [ 2 ]."
306,pagesOfTextToDisplay-metadataToHighlight.json,4.4725529733113945e-05,306,"It has superior performance on structural image transcription task compared to other general-purpose approaches, and also captures the structure of the image by producing a reading path following the image structure when transcribing."
93,pagesOfTextToDisplay-metadataToHighlight.json,4.44671131845098e-05,93,"To further demonstrate transcription on images with more complicated structure, we also construct the Multi-Line dataset that contains 4595 multi-line formulas, e.g., piecewise function, each line consisting of some complex formulas, e.g., multiple integral."
49,pagesOfTextToDisplay-metadataToHighlight.json,4.4463929953053594e-05,49,"The first is a straightforward one, i.e., STNM with Markov property , in which the spotlight placement follows a Markov chain."
28,pagesOfTextToDisplay-metadataToHighlight.json,4.438644100446254e-05,28,"Usually, existing approaches are designed in an encoder-decoder architecture, which consists of two components: (1) a CNN based encoder to capture and represent images as feature vectors that preserve their the semantic information [ 26 ]; (2) a RNN based decoder that decodes the features and generates output text sequences either directly [ 33 ], or attentively [ 36 ]."
308,pagesOfTextToDisplay-metadataToHighlight.json,4.400879697641358e-05,308,"First, STN learns to transcribe tokens directly with little prior knowledge of the image or specific languages."
90,pagesOfTextToDisplay-metadataToHighlight.json,4.3963118514511734e-05,90,"The Melody dataset contains pieces of music scores and their source code in LilyPond collected from the Internet 3 , mostly instrumental solos and choral pieces 2 Datasets are available at: http://home.ustc.edu.cn/~yxonic/stn_dataset.7z."
2,pagesOfTextToDisplay-metadataToHighlight.json,4.3567681132117286e-05,2,"To this end, in this paper, we propose a hierarchical S potlight T ranscribing N etwork (STN) framework followed by a two-stage ""where-to-what"" solution."
132,pagesOfTextToDisplay-metadataToHighlight.json,4.343679393059574e-05,132,"We follow the state-of-the-art image feature extractor design as in ResNet [ 11 ], adding residual connections between convolutional layers, together with ReLU activation [ 25 ] and batch normalization [ 15 ] to stabilize training, but removing the fully connected layers along with higher convolutional and pooling layers."
210,pagesOfTextToDisplay-metadataToHighlight.json,4.329158036853187e-05,210,"With the probability of each token as the output, the STN can be viewed as a stochastic policy that generates actions by sampling from the distribution π ( a | state t ; θ ) = P ( a | y 1 ,..., y t − 1 , x ; θ ) , where θ is the set of model parameters to be refined."
34,pagesOfTextToDisplay-metadataToHighlight.json,4.1136892832582816e-05,34,a8[ fis] dis16 a[ b c ... (a) Music score example f(x)=\frac{ \sqrt{ x-1 } }{x-2} (b) Formula example Figure 1: Some structural image examples.
58,pagesOfTextToDisplay-metadataToHighlight.json,4.096813790965825e-05,58,"Then for decoder selection, variations of recurrent neural networks (RNN), such as LSTM [ 12 ] and GRU [ 8 ], were utilized to generate the output text sequence, both of which are able to preserve long-term dependencies for text representations [ 32 ]."
268,pagesOfTextToDisplay-metadataToHighlight.json,4.082248779013753e-05,268,"All models are implemented by PyTorch 4 , and trained on a Linux server with four 2.0GHz Intel Xeon E5-2620 CPUs and a Tesla K20m GPU."
40,pagesOfTextToDisplay-metadataToHighlight.json,4.063925371156074e-05,40,"Second, content objects in structural images, even if they just take a small proportion, may carry much semantics."
8,pagesOfTextToDisplay-metadataToHighlight.json,3.933227708330378e-05,8,"KEYWORDS Structural image; Spotlight Transcribing Network; reinforcement learning ACM Reference Format: Yu Yin, Zhenya Huang, Enhong Chen, Qi Liu, Fuzheng Zhang, Xing Xie, and Guoping Hu."
0,pagesOfTextToDisplay-metadataToHighlight.json,3.9052494685165584e-05,0,"Transcribing Content from Structural Images with Spotlight Mechanism Yu Yin, Zhenya Huang Anhui Province Key Laboratory of Big Data Analysis and Application, University of Science and Technology of China {yxonic,huangzhy}@mail.ustc.edu.cn Enhong Chen ∗ Anhui Province Key Laboratory of Big Data Analysis and Application, University of Science and Technology of China cheneh@ustc.edu.cn Qi Liu Anhui Province Key Laboratory of Big Data Analysis and Application, University of Science and Technology of China qiliuql@ustc.edu.cn Fuzheng Zhang, Xing Xie Microsoft Research Asia {fuzzhang,xing.xie}@microsoft.com Guoping Hu iFLYTEK Research gphu@iflytek.com ABSTRACT Transcribing content from structural images, e.g., writing notes from music scores, is a challenging task as not only the content objects should be recognized, but the internal structure should also be preserved."
85,pagesOfTextToDisplay-metadataToHighlight.json,3.8050169678172097e-05,85,"Structural images refer to printed graphics that are not only a set of content objects, but also contain meaningful structure, i.e., object placement, following a certain grammar."
77,pagesOfTextToDisplay-metadataToHighlight.json,3.799671321758069e-05,77,"image pixels Melody 4208 70 82,834 19.7 15,602.7 Formula 61649 127 607,061 9.7 1,190.7 Multi-Line 4595 127 182,112 39.8 9,016.6 SVT 618 26 3,796 5.9 12,733.5 IIIT5K 3000 36 15,269 5.0 11,682.0 parameters according to task-specific reward signals."
189,pagesOfTextToDisplay-metadataToHighlight.json,3.691504389280453e-05,189,Sometimes longer spotlight history is needed for spotlight controlling on images with more complex structure.
138,pagesOfTextToDisplay-metadataToHighlight.json,3.5902623494621366e-05,138,"2) Considering the fact that the output history can be long, we embed the history before time step t into a hidden state vector h t by utilizing a variation of RNN — Gated Recurrent Unit (GRU), which preserves more long-term dependencies."
1,pagesOfTextToDisplay-metadataToHighlight.json,3.5666791518451646e-05,1,"Existing image recognition methods mainly work on images with simple content (e.g., text lines with characters), but are not capable to identify ones with more complex content (e.g., structured code), which often follow a fine-grained grammar."
213,pagesOfTextToDisplay-metadataToHighlight.json,3.444120739004575e-05,213,"Besides, we give -1 as the final reward if the output sequence does not compile, addressing grammar constraints by penalizing illegal outputs."
118,pagesOfTextToDisplay-metadataToHighlight.json,3.3572760003153235e-05,118,"Moreover, structural images are reversible, by which we mean that the token sequence is expected to reconstruct the original image using the corresponding compiler."
30,pagesOfTextToDisplay-metadataToHighlight.json,3.3449759939685464e-05,30,"Therefore, the problem of transcribing content from these structural images remains pretty much open."
86,pagesOfTextToDisplay-metadataToHighlight.json,3.303707126178779e-05,86,Content with its structure can often be described by a domain specific language and complied by the corresponding software.
41,pagesOfTextToDisplay-metadataToHighlight.json,3.243603350711055e-05,41,"For example, the note marked by blue box in Figure 1(a) is written as "" dis16 "" in LilyPond 1 , which means that the note is D# (""- is "" for sharp), and the note is a sixteenth note (denoted by "" 16 ""); the formula marked in Figure 1(b) means "" \sqrt{...} "" in T E X code, representing the square root operator, with the scope defined by curly braces."
