,sent_id,text,label
0,pagesOfTextToDisplay-metadataToHighlight.json.json_0,"Transcribing Content from Structural Images with Spotlight Mechanism Yu Yin, Zhenya Huang Anhui Province Key Laboratory of Big Data Analysis and Application, University of Science and Technology of China {yxonic,huangzhy}@mail.ustc.edu.cn Enhong Chen ∗ Anhui Province Key Laboratory of Big Data Analysis and Application, University of Science and Technology of China cheneh@ustc.edu.cn Qi Liu Anhui Province Key Laboratory of Big Data Analysis and Application, University of Science and Technology of China qiliuql@ustc.edu.cn Fuzheng Zhang, Xing Xie Microsoft Research Asia {fuzzhang,xing.xie}@microsoft.com Guoping Hu iFLYTEK Research gphu@iflytek.com ABSTRACT Transcribing content from structural images, e.g., writing notes from music scores, is a challenging task as not only the content objects should be recognized, but the internal structure should also be preserved.",
1,pagesOfTextToDisplay-metadataToHighlight.json.json_1,"Existing image recognition methods mainly work on images with simple content (e.g., text lines with characters), but are not capable to identify ones with more complex content (e.g., structured code), which often follow a fine-grained grammar.",
2,pagesOfTextToDisplay-metadataToHighlight.json.json_2,"To this end, in this paper, we propose a hierarchical S potlight T ranscribing N etwork (STN) framework followed by a two-stage ""where-to-what"" solution.",
3,pagesOfTextToDisplay-metadataToHighlight.json.json_3,"Specifically, we first decide ""where-tolook"" through a novel spotlight mechanism to focus on different areas of the original image following its structure.",
4,pagesOfTextToDisplay-metadataToHighlight.json.json_4,"Then, we decide ""what-to-write"" by developing a GRU based network with the spotlight areas for transcribing the content accordingly.",
5,pagesOfTextToDisplay-metadataToHighlight.json.json_5,"Moreover, we propose two implementations on the basis of STN, i.e., STNM and STNR, where the spotlight movement follows the Markov property and Recurrent modeling, respectively.",
6,pagesOfTextToDisplay-metadataToHighlight.json.json_6,We also design a reinforcement method to refine our STN framework by self-improving the spotlight mechanism.,
7,pagesOfTextToDisplay-metadataToHighlight.json.json_7,"We conduct extensive experiments on many structural image datasets, where the results clearly demonstrate the effectiveness of STN framework.",
8,pagesOfTextToDisplay-metadataToHighlight.json.json_8,"KEYWORDS Structural image; Spotlight Transcribing Network; reinforcement learning ACM Reference Format: Yu Yin, Zhenya Huang, Enhong Chen, Qi Liu, Fuzheng Zhang, Xing Xie, and Guoping Hu.",
9,pagesOfTextToDisplay-metadataToHighlight.json.json_9,2018.,
10,pagesOfTextToDisplay-metadataToHighlight.json.json_10,Transcribing Content from Structural Images with Spotlight Mechanism.,
11,pagesOfTextToDisplay-metadataToHighlight.json.json_11,In KDD '18: The 24th ACM SIGKDD International ∗ The corresponding author.,
12,pagesOfTextToDisplay-metadataToHighlight.json.json_12,Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.,
13,pagesOfTextToDisplay-metadataToHighlight.json.json_13,Copyrights for components of this work owned by others than ACM must be honored.,
14,pagesOfTextToDisplay-metadataToHighlight.json.json_14,Abstracting with credit is permitted.,
15,pagesOfTextToDisplay-metadataToHighlight.json.json_15,"To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.",
16,pagesOfTextToDisplay-metadataToHighlight.json.json_16,Request permissions from permissions@acm.org.,
17,pagesOfTextToDisplay-metadataToHighlight.json.json_17,"KDD '18, August 19–23, 2018, London, United Kingdom © 2018 Association for Computing Machinery.",
18,pagesOfTextToDisplay-metadataToHighlight.json.json_18,ACM ISBN 978-1-4503-5552-0/18/08. . .,
19,pagesOfTextToDisplay-metadataToHighlight.json.json_19,"$15.00 https://doi.org/10.1145/3219819.3219962 Conference on Knowledge Discovery & Data Mining, August 19–23, 2018, London, United Kingdom.",
20,pagesOfTextToDisplay-metadataToHighlight.json.json_20,"ACM, New York, NY, USA, 10 pages.",
21,pagesOfTextToDisplay-metadataToHighlight.json.json_21,https://doi.,
22,pagesOfTextToDisplay-metadataToHighlight.json.json_22,"org/10.1145/3219819.3219962 1 INTRODUCTION Transcribing content from images refers to recognizing semantic information in images into comprehensible forms (e.g., text) in computer vision [ 38 ].",
23,pagesOfTextToDisplay-metadataToHighlight.json.json_23,"It is an essential problem for computers to understand how humans communicate about what they see, which includes many tasks, such as reading text from scenes [ 17 , 40 ], writing notes from music scores [ 28 ] and recognizing formulas from pictures [6].",
24,pagesOfTextToDisplay-metadataToHighlight.json.json_24,"As it is crucial in many applications, e.g., image retrieval [ 5 , 29 ], online education systems [ 13 , 20 ] and assistant devices [ 9 ], much attention has been attracted from both academia and industry [38].",
25,pagesOfTextToDisplay-metadataToHighlight.json.json_25,"In the literature, there are many efforts for this transcribing problem, especially on text reading task.",
26,pagesOfTextToDisplay-metadataToHighlight.json.json_26,"Among them, the most representative one called Optical Character Recognition (OCR) has been extensively studied in many decades [ 14 ], which mainly follows rule-based solutions for generating texts from well-scanned documents [ 21 ].",
27,pagesOfTextToDisplay-metadataToHighlight.json.json_27,"Recently, researchers focus on a more general scene text recognition task, aiming to recognize texts from natural images [ 33 ].",
28,pagesOfTextToDisplay-metadataToHighlight.json.json_28,"Usually, existing approaches are designed in an encoder-decoder architecture, which consists of two components: (1) a CNN based encoder to capture and represent images as feature vectors that preserve their the semantic information [ 26 ]; (2) a RNN based decoder that decodes the features and generates output text sequences either directly [ 33 ], or attentively [ 36 ].",
29,pagesOfTextToDisplay-metadataToHighlight.json.json_29,"Though good performances have been achieved, previous studies mainly focus on the images with straightforward content (i.e., text with characters), while ignoring large proportion of structural images, where the content objects are well-formed in complex manners, e.g., music scores (Figure 1(a)) and formulas (Figure 1(b)).",
30,pagesOfTextToDisplay-metadataToHighlight.json.json_30,"Therefore, the problem of transcribing content from these structural images remains pretty much open.",
31,pagesOfTextToDisplay-metadataToHighlight.json.json_31,"In fact, there are many technical challenges along this line due to the unique characteristics of structural images.",
32,pagesOfTextToDisplay-metadataToHighlight.json.json_32,"First, different from natural images, where the text content is mostly placed in simple patterns, in structural images, the content objects usually follow a fine-grained grammar, and are organized in a more complex manner.",
33,pagesOfTextToDisplay-metadataToHighlight.json.json_33,"E.g., in Figure 1(a), notes from the music score are not only Research Track Paper KDD 2018, August 19 ‒ 23, 2018, London, United Kingdom 2643 KDD '18, August 19–23, 2018, London, United Kingdom Y. Yin et al.",
34,pagesOfTextToDisplay-metadataToHighlight.json.json_34,a8[ fis] dis16 a[ b c ... (a) Music score example f(x)=\frac{ \sqrt{ x-1 } }{x-2} (b) Formula example Figure 1: Some structural image examples.,
35,pagesOfTextToDisplay-metadataToHighlight.json.json_35,Left is a music bar from Cello Suite No.,
36,pagesOfTextToDisplay-metadataToHighlight.json.json_36,1 in G major by Bach; Right is a function formula from a high school math exercise.,
37,pagesOfTextToDisplay-metadataToHighlight.json.json_37,"placed simply from left to right, but the positions in the stave for each note are also specified, often with annotations added left or above.",
38,pagesOfTextToDisplay-metadataToHighlight.json.json_38,"A division formula in Figure 1(b) contains nested structure, where the equation components are placed at the left and right side of the equal sign, with two parts of the right-hand-side fraction placed above and below the middle line.",
39,pagesOfTextToDisplay-metadataToHighlight.json.json_39,"Thus, it is necessary for transcribing to not only capture the information from local areas, but also preserve the internal structure and organization of the content.",
40,pagesOfTextToDisplay-metadataToHighlight.json.json_40,"Second, content objects in structural images, even if they just take a small proportion, may carry much semantics.",
41,pagesOfTextToDisplay-metadataToHighlight.json.json_41,"For example, the note marked by blue box in Figure 1(a) is written as "" dis16 "" in LilyPond 1 , which means that the note is D# (""- is "" for sharp), and the note is a sixteenth note (denoted by "" 16 ""); the formula marked in Figure 1(b) means "" \sqrt{...} "" in T E X code, representing the square root operator, with the scope defined by curly braces.",
42,pagesOfTextToDisplay-metadataToHighlight.json.json_42,"Thus, it is very challenging to transcribe the complete content from an area containing such a informative object, compared to just one character in tasks such as scene text recognition.",
43,pagesOfTextToDisplay-metadataToHighlight.json.json_43,"Third, there exist plenty of similar objects puzzling the transcribing task, e.g., a sixteenth note (blue in Figure 1(a)) just contains one more flag on the stem than an eighth note (red), while notes with same duration and different pitches are almost identical except for their positioning.",
44,pagesOfTextToDisplay-metadataToHighlight.json.json_44,This characteristic requires a careful design for the transcribing.,
45,pagesOfTextToDisplay-metadataToHighlight.json.json_45,"To address the above challenges, following the observation on human transcribing process, i.e., first find out where to look, then write down the content, we present a two-stage ""where-to-what"" solution and propose a hierarchical framework called the S potlighted T ranscribing N etwork (STN) for transcribing content from structural images.",
46,pagesOfTextToDisplay-metadataToHighlight.json.json_46,"Specifically, after encoding images as features vectors, in our decoder component, we first propose a spotlight module with a novel mechanism to handle the ""where-to-look"" problem and decide a reading path focusing on areas of the original image following its internal structure.",
47,pagesOfTextToDisplay-metadataToHighlight.json.json_47,"Then, based on the learned spotlights areas, we aim for ""what-to-write"" problem and develop a GRU based network for transcribing the semantic content from the local spotlight areas.",
48,pagesOfTextToDisplay-metadataToHighlight.json.json_48,"Moreover, we propose two implementations on the basis of the STN framework.",
49,pagesOfTextToDisplay-metadataToHighlight.json.json_49,"The first is a straightforward one, i.e., STNM with Markov property , in which the spotlight placement follows a Markov chain.",
50,pagesOfTextToDisplay-metadataToHighlight.json.json_50,"Comparatively, the second is a more sophisticated one, i.e., STNR with Recurrent modeling , which can track long-term characteristics of spotlight movements.",
51,pagesOfTextToDisplay-metadataToHighlight.json.json_51,"We also design a reinforcement method to refine STN, self-improving the spotlight mechanism.",
52,pagesOfTextToDisplay-metadataToHighlight.json.json_52,"We conduct extensive experiments on real-world structural image datasets, where the results clearly demonstrate the effectiveness of the STN framework.",
53,pagesOfTextToDisplay-metadataToHighlight.json.json_53,"1 A domain specific language for music notation, http://lilypond.org/ 2 RELATED WORK The related research topics to our concerns can be classified into the following three categories: encoder-decoder system, attention mechanism, and reinforcement learning.",
54,pagesOfTextToDisplay-metadataToHighlight.json.json_54,"2.1 Encoder-Decoder System The encoder-decoder system is a general framework, which has been applied to many applications, such as neural machine translation [ 3 , 7 ] and image captioning [ 33 , 36 ].",
55,pagesOfTextToDisplay-metadataToHighlight.json.json_55,"Generally, the system has two separate parts, one encoder for representing and encoding the input information into a feature vector, and one decoder for generating the output sequence according to the encoded representation.",
56,pagesOfTextToDisplay-metadataToHighlight.json.json_56,"Due to its remarkable performance, many efforts have been made to apply it to scene text recognition [ 35 ], aiming at transcribing texts from natural images.",
57,pagesOfTextToDisplay-metadataToHighlight.json.json_57,"Specifically, for encoder design, representative works leveraged deep CNN based networks, which have been the most popular methods due to their performance on hierarchical feature extraction [ 26 ], to learn the information encodings from images [ 16 ].",
58,pagesOfTextToDisplay-metadataToHighlight.json.json_58,"Then for decoder selection, variations of recurrent neural networks (RNN), such as LSTM [ 12 ] and GRU [ 8 ], were utilized to generate the output text sequence, both of which are able to preserve long-term dependencies for text representations [ 32 ].",
59,pagesOfTextToDisplay-metadataToHighlight.json.json_59,"The whole architecture is end-to-end, which show the effectiveness in practice [30].",
60,pagesOfTextToDisplay-metadataToHighlight.json.json_60,"2.2 Attention Mechanism However, in the original encoder-decoder systems, encoding the whole input into one vector usually makes the encoded information of images clumsy and confusing for the decoder to read from, leading to unsatisfactory transcription [ 22 ].",
61,pagesOfTextToDisplay-metadataToHighlight.json.json_61,"To improve the encoderdecoder models addressing this problem, inspired by human visual system, researchers have tried to propose many attention mechanisms to highlight different parts of the encoder output by assigning weights to encoding vectors in each step of text generation [ 3 , 24 , 36 ] or sequential prediction [ 31 , 39 ].",
62,pagesOfTextToDisplay-metadataToHighlight.json.json_62,"For example, Bahdanau et al. [",
63,pagesOfTextToDisplay-metadataToHighlight.json.json_63,3 ] proposed a way to jointly generate and align words using attention mechanism.,
64,pagesOfTextToDisplay-metadataToHighlight.json.json_64,Xu et al. [,
65,pagesOfTextToDisplay-metadataToHighlight.json.json_65,36 ] proposed soft and hard attention mechanisms for image captioning.,
66,pagesOfTextToDisplay-metadataToHighlight.json.json_66,Lee et al. [,
67,pagesOfTextToDisplay-metadataToHighlight.json.json_67,19 ] used an attention-based encoder-decoder system for character recognition problems.,
68,pagesOfTextToDisplay-metadataToHighlight.json.json_68,Our work improves the previous studies mainly from the following two aspects.,
69,pagesOfTextToDisplay-metadataToHighlight.json.json_69,"First, the attention weights are usually calculated by the correspondence between outputs and the whole content, which let the models know ""what"" to look but not ""where"" to look.",
70,pagesOfTextToDisplay-metadataToHighlight.json.json_70,"In our work, we propose a novel spotlight mechanism to directly find a reading path tracking the image structure for transcribing.",
71,pagesOfTextToDisplay-metadataToHighlight.json.json_71,"Second, previous decoding process has one RNN for learning attentions and transcribing simultaneously, which may cause some confusion for transcription, while our framework models spotlighting and transcribing with two separate facilities, avoiding the confusion between two sequences.",
72,pagesOfTextToDisplay-metadataToHighlight.json.json_72,"2.3 Reinforcement Learning Deep reinforcement learning is a kind of state-of-the-art technique, which has shown superior abilities in many fields, such as gaming and robotics [ 1 ].",
73,pagesOfTextToDisplay-metadataToHighlight.json.json_73,"The main idea of them is to learn and refine model Research Track Paper KDD 2018, August 19 ‒ 23, 2018, London, United Kingdom 2644 Transcribing Content from Structural Images with Spotlight Mechanism KDD '18, August 19–23, 2018, London, United Kingdom 0 20 40 Content length 0 2 4 6 8 Percentage (%) (a) Melody 0 20 40 Content length 0 5 10 15 20 Percentage (%) (b) Formula 0 50 100 Content length 0 1 2 3 4 Percentage (%) (c) Multi-Line 0 20 40 Content length 0 5 10 15 20 Percentage (%) (d) SVT 0 20 40 Content length 0 5 10 15 Percentage (%) (e) IIIT5K Figure 2: Comparison of structural image (blue) and scene text recognition datasets (red) on content length distribution.",
74,pagesOfTextToDisplay-metadataToHighlight.json.json_74,Table 1: The statistics of the datasets.,
75,pagesOfTextToDisplay-metadataToHighlight.json.json_75,Dataset Image count Token space Token count Avg.,
76,pagesOfTextToDisplay-metadataToHighlight.json.json_76,tokens per image Avg.,
77,pagesOfTextToDisplay-metadataToHighlight.json.json_77,"image pixels Melody 4208 70 82,834 19.7 15,602.7 Formula 61649 127 607,061 9.7 1,190.7 Multi-Line 4595 127 182,112 39.8 9,016.6 SVT 618 26 3,796 5.9 12,733.5 IIIT5K 3000 36 15,269 5.0 11,682.0 parameters according to task-specific reward signals.",
78,pagesOfTextToDisplay-metadataToHighlight.json.json_78,"For example, Ranzato et al. [",
79,pagesOfTextToDisplay-metadataToHighlight.json.json_79,"27 ] used the whole sequence metrics to guide the sequence generation, using REINFORCE method; Bahdanau et al. [",
80,pagesOfTextToDisplay-metadataToHighlight.json.json_80,"2 ] utilized the actor-critic algorithm for sequence prediction, refining the model to improve sentence BLEU score.",
81,pagesOfTextToDisplay-metadataToHighlight.json.json_81,"3 PRELIMINARIES In this section, we first give a clear definition of structural images, and introduce the structural image datasets used in this paper.",
82,pagesOfTextToDisplay-metadataToHighlight.json.json_82,Then we discuss the crucial differences between structural image transcribing and typical scene text recognition with exclusive data analysis.,
83,pagesOfTextToDisplay-metadataToHighlight.json.json_83,"At last, we give the formal definition of the structural image transcription problem.",
84,pagesOfTextToDisplay-metadataToHighlight.json.json_84,"3.1 Data Description In this paper, we mainly focus on transcribing content from structural images.",
85,pagesOfTextToDisplay-metadataToHighlight.json.json_85,"Structural images refer to printed graphics that are not only a set of content objects, but also contain meaningful structure, i.e., object placement, following a certain grammar.",
86,pagesOfTextToDisplay-metadataToHighlight.json.json_86,Content with its structure can often be described by a domain specific language and complied by the corresponding software.,
87,pagesOfTextToDisplay-metadataToHighlight.json.json_87,"Typical structural images include music scores, formulas and flow charts, etc.,",
88,pagesOfTextToDisplay-metadataToHighlight.json.json_88,"which can be described in music notation, T E X and UML code, respectively.",
89,pagesOfTextToDisplay-metadataToHighlight.json.json_89,"We exploit two real-world datasets, i.e., Melody and Formula , along with one synthetic dataset Multi-Line , specifically for the structural image transcription task 2 .",
90,pagesOfTextToDisplay-metadataToHighlight.json.json_90,"The Melody dataset contains pieces of music scores and their source code in LilyPond collected from the Internet 3 , mostly instrumental solos and choral pieces 2 Datasets are available at: http://home.ustc.edu.cn/~yxonic/stn_dataset.7z.",
91,pagesOfTextToDisplay-metadataToHighlight.json.json_91,"3 http://web.mit.edu/music21/ written by Bach, split into 1 to 4 bar length, forming 4208 imagecode pairs.",
92,pagesOfTextToDisplay-metadataToHighlight.json.json_92,"The Formula dataset is collected from Zhixue.com, an online educational system, which contains 61649 printed formulas from high school math exercises, with their corresponding T E X code.",
93,pagesOfTextToDisplay-metadataToHighlight.json.json_93,"To further demonstrate transcription on images with more complicated structure, we also construct the Multi-Line dataset that contains 4595 multi-line formulas, e.g., piecewise function, each line consisting of some complex formulas, e.g., multiple integral.",
94,pagesOfTextToDisplay-metadataToHighlight.json.json_94,We summarize some basic statistics of these datasets in Table 1.,
95,pagesOfTextToDisplay-metadataToHighlight.json.json_95,We now conduct deep analysis to show the unique characteristics of the structural image transcription task compared to traditional scene text recognition.,
96,pagesOfTextToDisplay-metadataToHighlight.json.json_96,"Specifically, we compare our datasets with two commonly used datasets for scene text recognition, i.e., SVT [ 34 ] and IIIT5K [ 23 ], and conclude three main differences.",
97,pagesOfTextToDisplay-metadataToHighlight.json.json_97,"First, structural image transcription needs to preserve more information: other than just objects, how they are organized should also be transcribed.",
98,pagesOfTextToDisplay-metadataToHighlight.json.json_98,"As shown in Table 1 and Figure 2, our datasets contain significantly longer content in relatively small images.",
99,pagesOfTextToDisplay-metadataToHighlight.json.json_99,"Sequences longer than 10 tokens taking 75.0%, 30.4% and 99.9% of Melody, Formula and Multi-Line datasets, respectively.",
100,pagesOfTextToDisplay-metadataToHighlight.json.json_100,"However, only 1.9% in SVT and 2.7% in IIIT5K have more than 10 character long sequences.",
101,pagesOfTextToDisplay-metadataToHighlight.json.json_101,"In addition, Melody, Formula and Multi-Line contain in average 1.26, 8.15 and 4.14 tokens every 1000 pixels, while SVT and IIIT5k only contain 0.46 and 0.43 characters, respectively, which indicates that each proportion of an image contains more information to be transcribed, along with the informative structure.",
102,pagesOfTextToDisplay-metadataToHighlight.json.json_102,"Second, the output space and count in our datasets are often larger than SVT and IIIT5K, as shown in Table 1.",
103,pagesOfTextToDisplay-metadataToHighlight.json.json_103,"Hence, it is even more complicated to transcribe content from structural images compared to text recognition.",
104,pagesOfTextToDisplay-metadataToHighlight.json.json_104,"Third, structural image transcription process is reversible, meaning the corresponding code should be able to compile and regenerate the original image, which is not necessary or possible for traditional scene text recognition.",
105,pagesOfTextToDisplay-metadataToHighlight.json.json_105,"In summary, the above analysis clearly shows that the structural image transcription problem is quite different from traditional scene text recognition tasks.",
106,pagesOfTextToDisplay-metadataToHighlight.json.json_106,"As a result, it is necessary to design a new approach that better fits this problem.",
107,pagesOfTextToDisplay-metadataToHighlight.json.json_107,"3.2 Problem Definition In this subsection, we formally introduce the structural image transcription problem.",
108,pagesOfTextToDisplay-metadataToHighlight.json.json_108,"In our image transcribing applications, we are given structural images and their corresponding source code.",
109,pagesOfTextToDisplay-metadataToHighlight.json.json_109,"Each Research Track Paper KDD 2018, August 19 ‒ 23, 2018, London, United Kingdom 2645 KDD '18, August 19–23, 2018, London, United Kingdom Y. Yin et al.",
110,pagesOfTextToDisplay-metadataToHighlight.json.json_110,"Transcribing Decoder Transcription Module h t-1 t - 1 control control CNN (dis) (16) (a) V Image Encoder Spotlight Module control y t y t + 1 y t - 1 h t + 1 h t h t - 1 s t s t + 1 s t - 1 sc t sc t + 1 sc Figure 3: The STN model architecture consists of two main parts: 1) a convolutional image feature extractor as the encoder, and 2) the transcribing decoder.",
111,pagesOfTextToDisplay-metadataToHighlight.json.json_111,"At the decoding stage, the spotlight module is first engaged to handle the ""where-to-look"" problem.",
112,pagesOfTextToDisplay-metadataToHighlight.json.json_112,"Afterwards, the transcription module finds out ""what-to-write"" by utilizing the spotlighted information from the encoder, generating the transcribed content one token at a time.",
113,pagesOfTextToDisplay-metadataToHighlight.json.json_113,"input image x is a one-channel gray-scale image with width W and height H , containing content such as music notations or printed formulas.",
114,pagesOfTextToDisplay-metadataToHighlight.json.json_114,"For each image, the expected output, i.e., its source code, is given as a token sequence y = { y 1 , y 2 ,..., y T } , where T is the length of token sequence.",
115,pagesOfTextToDisplay-metadataToHighlight.json.json_115,"Each y t can be a LilyPond notation ( c , fis , . . . )",
116,pagesOfTextToDisplay-metadataToHighlight.json.json_116,"in music score transcribing task, or a T E X token ( x , \frac , . . . )",
117,pagesOfTextToDisplay-metadataToHighlight.json.json_117,in formula transcribing task.,
118,pagesOfTextToDisplay-metadataToHighlight.json.json_118,"Moreover, structural images are reversible, by which we mean that the token sequence is expected to reconstruct the original image using the corresponding compiler.",
119,pagesOfTextToDisplay-metadataToHighlight.json.json_119,"Therefore, the problem can be defined as: Definition 3.1. (",
120,pagesOfTextToDisplay-metadataToHighlight.json.json_120,Structural Image Transcription Problem ).,
121,pagesOfTextToDisplay-metadataToHighlight.json.json_121,"Given a structural W × H image x , our goal is to transcribe the content from it as a sequence ˆ y = { ˆ y 1 , ˆ y 2 ,..., ˆ y T } as close as possible to the source code sequence y , where each ˆ y t is the predicted token taking from the specific language corresponding to the image.",
122,pagesOfTextToDisplay-metadataToHighlight.json.json_122,"4 SPOTLIGHTED TRANSCRIBING NETWORK In this section, we introduce the Spotlighted Transcribing Network (STN) framework in detail.",
123,pagesOfTextToDisplay-metadataToHighlight.json.json_123,First we give an overview of the model architecture.,
124,pagesOfTextToDisplay-metadataToHighlight.json.json_124,Then we describe all the details of our proposed spotlight mechanism in following sections.,
125,pagesOfTextToDisplay-metadataToHighlight.json.json_125,Finally we discuss the training process of STN with reinforcement learning for refinement.,
126,pagesOfTextToDisplay-metadataToHighlight.json.json_126,"4.1 Model Overview Figure 3 shows the overall architecture of Spotlighted Transcribing Network (STN), which consists of two main components: (1) a convolutional feature extractor network as the encoder, which learns the visual representations V from the input image x ; (2) a hierarchical transcribing decoder, which we mainly focus on in this work.",
127,pagesOfTextToDisplay-metadataToHighlight.json.json_127,"Mimicking human reading process, the decoder first takes the encoded image information V and find out ""where-to-look"" by shedding spotlight on it, following the learned reading path, then generates the token sequence y , by predicting one token at a time using a GRU-based output network, solving the ""what-to-write"" problem.",
128,pagesOfTextToDisplay-metadataToHighlight.json.json_128,"In the following subsections, we will explain how each part of the STN works in detail.",
129,pagesOfTextToDisplay-metadataToHighlight.json.json_129,4.2 Image Encoder The encoder part of STN is for extracting and embedding information from the image.,
130,pagesOfTextToDisplay-metadataToHighlight.json.json_130,"Instead of embedding the complete image x into one vector, which may cause a loss in structural information [ 36 ], we extract a set of feature vectors V , each of which is a D -dimensional representation corresponding to a part of the image: V = { V ( i , j ) : i = 1 ,..., W ′ , j = 1 ,..., H ′ } , V ( i , j ) ∈ R D .",
131,pagesOfTextToDisplay-metadataToHighlight.json.json_131,"A deep convolutional neural network (CNN) is used as the feature extractor to capture high-level semantic information, which we denote as f (· ; θ f ) .",
132,pagesOfTextToDisplay-metadataToHighlight.json.json_132,"We follow the state-of-the-art image feature extractor design as in ResNet [ 11 ], adding residual connections between convolutional layers, together with ReLU activation [ 25 ] and batch normalization [ 15 ] to stabilize training, but removing the fully connected layers along with higher convolutional and pooling layers.",
133,pagesOfTextToDisplay-metadataToHighlight.json.json_133,"As a result, we construct an extractor network that takes an image x , outputs a 3 dimensional tensor V ( W ′ × H ′ × D ): V = f ( x ; θ f ) , (1) Research Track Paper KDD 2018, August 19 ‒ 23, 2018, London, United Kingdom 2646 Transcribing Content from Structural Images with Spotlight Mechanism KDD '18, August 19–23, 2018, London, United Kingdom where vector V ( i , j ) at each location ( i , j ) represents the local semantic information.",
134,pagesOfTextToDisplay-metadataToHighlight.json.json_134,"The output tensor also preserves spatial and contextual information, with the property that adjacent vectors representing neighboring parts of the image.",
135,pagesOfTextToDisplay-metadataToHighlight.json.json_135,This allows the decoder module to use the image information selectively with both content and location in mind.,
136,pagesOfTextToDisplay-metadataToHighlight.json.json_136,"4.3 Transcribing Decoder The transcribing decoder of STN, as in typical encoder-decoder architecture, generates one token at a time, by giving its conditional probability over the encoder output V and all the previous outputs { y 1 ,..., y t − 1 } at each time step t .",
137,pagesOfTextToDisplay-metadataToHighlight.json.json_137,"Hence, we can denote the probability of a decoder yielding a sequence y as: P ( y | x ) = T Ö t = 1 P ( y t | y 1 ,..., y t − 1 , V ) . (",
138,pagesOfTextToDisplay-metadataToHighlight.json.json_138,"2) Considering the fact that the output history can be long, we embed the history before time step t into a hidden state vector h t by utilizing a variation of RNN — Gated Recurrent Unit (GRU), which preserves more long-term dependencies.",
139,pagesOfTextToDisplay-metadataToHighlight.json.json_139,"Formally, at time step t , the hidden state for output history h t is updated based on the last output item y t − 1 and the previous output history h t − 1 , by an GRU network GRU (· ; θ h ) : h t = GRU ( y t − 1 , h t − 1 ; θ h ) . (",
140,pagesOfTextToDisplay-metadataToHighlight.json.json_140,"3) For image part, the visual representation V we get as the encoder output carries enough semantic information, but as a whole it can be confounding for the decoder to comprehend, and thus needs careful selection [ 36 ].",
141,pagesOfTextToDisplay-metadataToHighlight.json.json_141,"To deal with this problem, we mimic what human do when reading images: focus on one spot at a time, write down content, then focus on a next spot following the image structure [ 4 ].",
142,pagesOfTextToDisplay-metadataToHighlight.json.json_142,"Along this line, we propose a module with novel spotlight mechanism, where at each time step, we only focus on information around a certain spotlight center.",
143,pagesOfTextToDisplay-metadataToHighlight.json.json_143,"We refer to the spotlight center position as s t at time step t , and the spotlighted information as spotlight context sc t .",
144,pagesOfTextToDisplay-metadataToHighlight.json.json_144,"Further details on how to get focused spotlight context are described in Section 4.4, while how to move the spotlight following the structure is described in Section 4.5.",
145,pagesOfTextToDisplay-metadataToHighlight.json.json_145,"With embedded history h t , and spotlight context sc t , together with current spotlight position s t , the conditional probability of output token at time t can then be parameterized as follows: P ( y t | y 1 ,..., y t − 1 , V ) = Softmax ( d ( h t ⊕ sc t ⊕ s t ; θ d )) , (4) where d (· ; θ d ) is a transformation function (e.g. a feed-forward neural network) that outputs a vocabulary-sized vector, and ⊕ represents the operation that concatenates two vectors.",
146,pagesOfTextToDisplay-metadataToHighlight.json.json_146,"The overall transcription loss L on an image-sequence pair is then defined as the negative log likelihood of the token sequence over the image: L = T Õ t = 1 − log P ( y t | y 1 ,..., y t − 1 , V ) . (",
147,pagesOfTextToDisplay-metadataToHighlight.json.json_147,"5) With all the calculation being deterministic and differentiable, the model can be optimized through standard back-propagation. """,
148,pagesOfTextToDisplay-metadataToHighlight.json.json_148, 1 1 1 . . .,
149,pagesOfTextToDisplay-metadataToHighlight.json.json_149,2 2 2 . . . . . . . . . . . . . . .,
150,pagesOfTextToDisplay-metadataToHighlight.json.json_150,W ′ W ′ W ′ . . .,
151,pagesOfTextToDisplay-metadataToHighlight.json.json_151,I − x t x t x t . . .,
152,pagesOfTextToDisplay-metadataToHighlight.json.json_152,x t x t x t . . . . . . . . . . . . . . .,
153,pagesOfTextToDisplay-metadataToHighlight.json.json_153,x t x t x t . . .,
154,pagesOfTextToDisplay-metadataToHighlight.json.json_154,X t  2 +  H ′ 1 2 . . .,
155,pagesOfTextToDisplay-metadataToHighlight.json.json_155,H ′ 1 2 . . . . . . . . . . . . . . .,
156,pagesOfTextToDisplay-metadataToHighlight.json.json_156,H ′ 1 2 . . .,
157,pagesOfTextToDisplay-metadataToHighlight.json.json_157,J − y t y t y t . . .,
158,pagesOfTextToDisplay-metadataToHighlight.json.json_158,y t y t y t . . . . . . . . . . . . . . .,
159,pagesOfTextToDisplay-metadataToHighlight.json.json_159,y t y t y t . . .,
160,pagesOfTextToDisplay-metadataToHighlight.json.json_160,Y t  2 # / σ 2 t Figure 4: Demonstration of the parallelized operation on assigning weights.,
161,pagesOfTextToDisplay-metadataToHighlight.json.json_161,"It should be clear that the element at each position ( i , j ) of the result matrix is [( i − x t ) 2 + ( j − y t ) 2 ]/ σ 2 t .",
162,pagesOfTextToDisplay-metadataToHighlight.json.json_162,"4.4 Spotlight Mechanism In this subsection, we describe how to get focused information of the input image, i.e., the spotlight context sc t , with our proposed spotlight mechanism.",
163,pagesOfTextToDisplay-metadataToHighlight.json.json_163,"How the spotlight moves through time is handled in a separate spotlight control module, and is described later in detail in Section 4.5.",
164,pagesOfTextToDisplay-metadataToHighlight.json.json_164,"As mentioned earlier, the visual embedding V is confounding for the decoder, and we want to focus on one spot at a time when generating output.",
165,pagesOfTextToDisplay-metadataToHighlight.json.json_165,"To achieve this goal, we propose a novel spotlight mechanism to mimic human focus directly, where at each time step, we only care about information around a certain location which we call a spotlight center, by ""shedding"" a spotlight around it.",
166,pagesOfTextToDisplay-metadataToHighlight.json.json_166,"More specifically, we define a spotlight handle s t = ( x t , y t , σ t ) T at each time step t to represent the spotlight, where ( x t , y t ) represents the center position of the spotlight, and σ t represents the radius of the spotlight.",
167,pagesOfTextToDisplay-metadataToHighlight.json.json_167,Inspired by Yang et al. [,
168,pagesOfTextToDisplay-metadataToHighlight.json.json_168,"37 ], we ""shed"" a spotlight by assigning weights to image representation vectors at each position, following a truncated Gaussian distribution centered at ( x t , y t ) , with the same variance σ t on both axis.",
169,pagesOfTextToDisplay-metadataToHighlight.json.json_169,"Formally, under the spotlight with handle s t = ( x t , y t , σ t ) T , the weights for each vector at position ( i , j ) at time step t , denoted as α ( i , j ) t , is proportional to the probability density at point ( i , j ) under Gaussian distribution: α ( i , j ) t ∼N(( i , j ) T | μ t , Σ t ) , (6) μ t = ( x t , y t ) T Σ t =  σ t 0 0 σ t  . (",
170,pagesOfTextToDisplay-metadataToHighlight.json.json_170,"7) Intuitively, the closer ( i , j ) is to the center ( x t , y t ) , the higher the weight should be, mimicking shedding a spotlight with radius σ t onto the location ( x t , y t ) .",
171,pagesOfTextToDisplay-metadataToHighlight.json.json_171,"To calculate the weight α ( i , j ) t of each position ( i , j ) while still make the process differentiable, we apply the definition of Gaussian distribution and rewrite the expression of α ( i , j ) t as: α ( i , j ) t = Softmax ( b t ) = exp ( b ( i , j ) t ) Í W ′ u = 1 Í H ′ v = 1 exp ( b ( u , v ) t ) , (8) b ( i , j ) t = − ( i − x t ) 2 + ( j − y t ) 2 σ 2 t , (9) where b measures how close the point ( i , j ) is to the center ( x t , y t ) , i.e., how important this point is, and α is thus a W ′ × H ′ matrix following the truncated Gaussian distribution for each point ( i , j ) , and can later be used as weights for each image feature vector.",
172,pagesOfTextToDisplay-metadataToHighlight.json.json_172,"To parallize the calculation of Equation (9), we perform a small trick as demonstrated in Figure 4.",
173,pagesOfTextToDisplay-metadataToHighlight.json.json_173,"We first construct two W ′ × H ′ matrices I and J in advance, each of them representing one Research Track Paper KDD 2018, August 19 ‒ 23, 2018, London, United Kingdom 2647 KDD '18, August 19–23, 2018, London, United Kingdom Y. Yin et al.",
174,pagesOfTextToDisplay-metadataToHighlight.json.json_174,coordinate.,
175,pagesOfTextToDisplay-metadataToHighlight.json.json_175,"Specifically, as shown in Figure 4, for each point ( i , j ) , we have I ( i , j ) = i and J ( i , j ) = j .",
176,pagesOfTextToDisplay-metadataToHighlight.json.json_176,"We also expand x t and y t as W ′ × H ′ matrices X t and Y t respectively, with same value for each element.",
177,pagesOfTextToDisplay-metadataToHighlight.json.json_177,"Therefore, Equation (9) can be written as the matrix form: b t = −[( I − X t ) 2 + ( J − Y t ) 2 ]/ σ 2 t (10) The focused information of the visual representation V at time step t can then be computed as a spotlight context vector sc t weighted by α ( i , j ) t according to current spotlight handle s t , i.e., the weighted sum of features at each position: sc t = W ′ Õ i = 1 H ′ Õ j = 1 α ( i , j ) t V ( i , j ) (11) Please note that the spotlight context sc t represents the information in the focused area at time step t , and should contain useful information specifically for transcribing at current time step.",
178,pagesOfTextToDisplay-metadataToHighlight.json.json_178,"By focusing directly on the correct spot, the transcription module therefore only cares about the local information, not confusing at areas with similar content all over the image.",
179,pagesOfTextToDisplay-metadataToHighlight.json.json_179,"4.5 Spotlight Control Now we discuss how to control the spotlight to find a proper reading path, following the image structure through the whole generation process.",
180,pagesOfTextToDisplay-metadataToHighlight.json.json_180,"Different from traditional attention strategy where both output sequence and attention behavior are embedded in one module, we see the spotlight movement (i.e., the value of the spotlight handle s t = ( x t , y t , σ t ) T at each time step t ) as a separate sequence devoted to following the image structure, and model this sequence with a standalone spotlight controlling module, without mixing the information with the output sequence.",
181,pagesOfTextToDisplay-metadataToHighlight.json.json_181,"We provide two implementations under the STN framework, i.e., the straightforward STNM with Markov property , and the more sophisticated STNR with Recurrent modeling , utilizing another GRU network.",
182,pagesOfTextToDisplay-metadataToHighlight.json.json_182,Each implementation models the spotlight handle sequences differently.,
183,pagesOfTextToDisplay-metadataToHighlight.json.json_183,STNM with Markov property.,
184,pagesOfTextToDisplay-metadataToHighlight.json.json_184,"With an assumption that is not far from reality, we can intuitively treat the spotlight handle sequence as a Markov process, i.e., current spotlight handle only depends on the previous handle, along with other internal states at current time step.",
185,pagesOfTextToDisplay-metadataToHighlight.json.json_185,"Treating the spotlight handle as a Markov process means the probability of choosing s t at time t does not rely on spotlight handles more than one step earlier, i.e.: P ( s t | s 1 ,..., s t − 1 ; ·) = P ( s t | s t − 1 ; ·) . (",
186,pagesOfTextToDisplay-metadataToHighlight.json.json_186,"12) To decide where to put the spotlight properly, the model also needs to know current internal states at time step t , including the spotlight context sc t − 1 which represents previous spotlighted region, and the history embedding h t which represents output history before time t .",
187,pagesOfTextToDisplay-metadataToHighlight.json.json_187,"Thus, we can use a feed-forward neural network n (· ; θ n ) to model the choice of s t (Figure 5 (a)) as: s t = n ( s t − 1 ⊕ sc t − 1 ⊕ h t ; θ n ) (13) The way we model the sequence is simple and time-independent, which makes it easier for the controlling module to train.",
188,pagesOfTextToDisplay-metadataToHighlight.json.json_188,STNR with Recurrent modeling.,
189,pagesOfTextToDisplay-metadataToHighlight.json.json_189,Sometimes longer spotlight history is needed for spotlight controlling on images with more complex structure.,
190,pagesOfTextToDisplay-metadataToHighlight.json.json_190,"To track the image structure as a sequence with long-term dependency, we propose another GRU network (a) Markovian control module. (",
191,pagesOfTextToDisplay-metadataToHighlight.json.json_191,b) Recurrent control module.,
192,pagesOfTextToDisplay-metadataToHighlight.json.json_192,t - 1 s t - 1 s t s t s t s t + 1 s t h t h t + 1 h Figure 5: The spotlight control module implementations.,
193,pagesOfTextToDisplay-metadataToHighlight.json.json_193,"GRU (· ; θ д ) to track the spotlight history, and a fully connected layer c (· ; θ c ) to generate next spotlight handle (Figure 5 (b)).",
194,pagesOfTextToDisplay-metadataToHighlight.json.json_194,"Specifically, at time step t , with last spotlight history embedding denoted as e t , the current spotlight handle s t at time t is calculated as: s t = c ( e t ⊕ sc t − 1 ⊕ h t ; θ c ) (14) and the history embedding is updated by: e t = GRU ( s t − 1 , e t − 1 ; θ д ) (15) Through a separate module specifically for spotlight control, STN gains two advantages over the traditional attention mechanism.",
195,pagesOfTextToDisplay-metadataToHighlight.json.json_195,"First, STN focuses on local areas by design, and the model will only have to learn where to focus and what to transcribe, while the attention model have to first learn to focus, then learn what to focus on.",
196,pagesOfTextToDisplay-metadataToHighlight.json.json_196,"Second, modeling reading and writing process as two separate sequences, with a standalone module dedicated for the ""where-to-look"" problem, STN is capable for directly learning a reading path on structural images apart from generating the output sequences, which enables our model to track the image structure more closely compared to attentive models where attentions and transcribing process are modeled together in only one network.",
197,pagesOfTextToDisplay-metadataToHighlight.json.json_197,"4.6 Training and Refining STN Parameters to be updated in both implementations comes from three parts: the encoder parameters θ f , the decoder parameters { θ h , θ d } , and parameters in the spotlight control module, which are θ n in STNM and { θ c , θ д } in STNR.",
198,pagesOfTextToDisplay-metadataToHighlight.json.json_198,"The parameters are updated to minimize the total transcription loss L (Equation (5)) through a gradient descent algorithm, which we choose the Adam optimizer [ 18 ].",
199,pagesOfTextToDisplay-metadataToHighlight.json.json_199,More detailed settings are presented in the experiment section.,
200,pagesOfTextToDisplay-metadataToHighlight.json.json_200,"Though our model is differentiable, and can be optimized through back-propagation methods, directly training to fit the label suffers from some specific aspects in the image transcribing task.",
201,pagesOfTextToDisplay-metadataToHighlight.json.json_201,"Firstly, the model has to jointly learn two different sequences with only one of them directly supervised, which may result in inaccurate reading path.",
202,pagesOfTextToDisplay-metadataToHighlight.json.json_202,"Second, the given token sequence may only be one of the many correct ones that all regenerates the original image.",
203,pagesOfTextToDisplay-metadataToHighlight.json.json_203,"For instance, in LilyPond notation, we can optionally omit duration for notes at same length with their predecessors.",
204,pagesOfTextToDisplay-metadataToHighlight.json.json_204,Fitting to only one of the correct sequences lets down the model even when it achieves good strategies.,
205,pagesOfTextToDisplay-metadataToHighlight.json.json_205,"Fortunately, in structural image transcription problems, we have an advantage that the process is reversible, meaning given the transcribed sequence, we can use a compiler to reconstruct Research Track Paper KDD 2018, August 19 ‒ 23, 2018, London, United Kingdom 2648 Transcribing Content from Structural Images with Spotlight Mechanism KDD '18, August 19–23, 2018, London, United Kingdom the image.",
206,pagesOfTextToDisplay-metadataToHighlight.json.json_206,"With the guidance of this, we can further refine our model using reinforcement learning, by regarding our sequential generation as a decision making problem, viewing it as a Markov Decision Process (MDP) [ 2 ].",
207,pagesOfTextToDisplay-metadataToHighlight.json.json_207,"Formally, we define the state , action and reward of the MDP as follows: State: View our problem as outputting the probability of items at each time step conditioned by the image and previous generations, the environment state at time step t as the combination of the image x and the output history { y 1 ,..., y t − 1 } , which is exactly the inputs of the STN.",
208,pagesOfTextToDisplay-metadataToHighlight.json.json_208,"Therefore, instead of directly using the environment state, we use the internal states (combined and denoted as state t ) in STN framework as MDP states.",
209,pagesOfTextToDisplay-metadataToHighlight.json.json_209,Action: Taking action a t is defined as generating the token y t at time step t .,
210,pagesOfTextToDisplay-metadataToHighlight.json.json_210,"With the probability of each token as the output, the STN can be viewed as a stochastic policy that generates actions by sampling from the distribution π ( a | state t ; θ ) = P ( a | y 1 ,..., y t − 1 , x ; θ ) , where θ is the set of model parameters to be refined.",
211,pagesOfTextToDisplay-metadataToHighlight.json.json_211,"Reward: After taking the action, a reward signal r is received.",
212,pagesOfTextToDisplay-metadataToHighlight.json.json_212,"Here we define the reward r t as 0 when the generation is not finished at time step t , or the pixel similarity between the reconstruction image and the original image after the whole generation process finished.",
213,pagesOfTextToDisplay-metadataToHighlight.json.json_213,"Besides, we give -1 as the final reward if the output sequence does not compile, addressing grammar constraints by penalizing illegal outputs.",
214,pagesOfTextToDisplay-metadataToHighlight.json.json_214,"The goal is to maximize the sum of the discounted rewards from each time t , i.e., the return: R t = T Õ k = t γ k r k . (",
215,pagesOfTextToDisplay-metadataToHighlight.json.json_215,"16) We further define a value network v (· ; θ v ) for estimation of the expected return from each state t , which is a feed-forward network with the same input as the STN output layer d .",
216,pagesOfTextToDisplay-metadataToHighlight.json.json_216,"The estimated value v t , i.e., the expected return, at time step t is then v t = v ( h t ⊕ sc t ⊕ s t ; θ v ) . (",
217,pagesOfTextToDisplay-metadataToHighlight.json.json_217,"17) With a stochastic policy together with a value network, we can apply the actor-critic algorithm [ 2 ] to our sequence generation problem, with the policy network trained using policy gradient at each time step t as: ∇ θ = log π ( a | state t ; θ )( R t − v t ) , (18) and the value network trained by optimizing the distance between the estimated value and actual return: L v alue = || v t − R t || 2 2 .",
218,pagesOfTextToDisplay-metadataToHighlight.json.json_218,"As the whole model is complicated, directly applying reinforcement learning to the model suffers from the large searching space.",
219,pagesOfTextToDisplay-metadataToHighlight.json.json_219,"Through experiments we notice that, after supervised training, the image extractor and the output history embedding modules have both been trained properly, and it is more important for our framework to have a better reading path to make precise predictions, which indicates that refining the spotlight module is most beneficial.",
220,pagesOfTextToDisplay-metadataToHighlight.json.json_220,"Therefore, at reinforcement stage, we only optimize parameters from the spotlight control module ( θ n in STNM, θ c and θ д in STNR), along with those from the output layer ( θ o ), and omit θ f and θ h , which reduces the variance when applying reinforcement learning algorithms, and get better improvements.",
221,pagesOfTextToDisplay-metadataToHighlight.json.json_221,"With this train-and-refine procedure, our model can learn a reasonable reading path on structural images, focusing on different parts following the image structure when transcribing, and get superior transcription results, as our experimental results show in the next section.",
222,pagesOfTextToDisplay-metadataToHighlight.json.json_222,"5 EXPERIMENTS In this section, we conduct extensive experiments to demonstrate the effectiveness of STN model from various aspects: (1) the transcribing performance; (2) the validation loss demonstrating the model sensitivity; (3) the spotlight visualization of STN.",
223,pagesOfTextToDisplay-metadataToHighlight.json.json_223,5.1 Experimental Setup 5.1.1 Data partition and preprocessing.,
224,pagesOfTextToDisplay-metadataToHighlight.json.json_224,"We partition all our datasets, i.e., Melody , Formula and Multi-Line , into 60%/40%, 70%/30%, 80%/20%, 90%/10% as training/testing sets, respectively, to test model performance at different data sparsity.",
225,pagesOfTextToDisplay-metadataToHighlight.json.json_225,"From each training set, we also sample 10% images as validation set.",
226,pagesOfTextToDisplay-metadataToHighlight.json.json_226,"The images are randomly scaled and cropped for stable training, and ground-truth source code is cut into token sequences in the corresponding language to reduce searching space.",
227,pagesOfTextToDisplay-metadataToHighlight.json.json_227,5.1.2 STN setting.,
228,pagesOfTextToDisplay-metadataToHighlight.json.json_228,"We now specify the model setup in STN, including image encoder, transcription decoder and reinforcement module.",
229,pagesOfTextToDisplay-metadataToHighlight.json.json_229,"For STN image encoder, we use a variation of ResNet [ 11 ], and set the encoded vector width as 128.",
230,pagesOfTextToDisplay-metadataToHighlight.json.json_230,"For its transcribing decoder, we set the output history embedding h t , and the spotlight history embedding e t as the same dimensions of 128, respectively.",
231,pagesOfTextToDisplay-metadataToHighlight.json.json_231,"The value network used at the reinforcement stage is a two-layer fullyconnected neural network, with the hidden layer also sized at 128.",
232,pagesOfTextToDisplay-metadataToHighlight.json.json_232,5.1.3 Training setting.,
233,pagesOfTextToDisplay-metadataToHighlight.json.json_233,"To set up the training process, we initialize all parameters in STN following [ 10 ].",
234,pagesOfTextToDisplay-metadataToHighlight.json.json_234,"Each parameter is sampled from U  − p 6 /( n in + n out ) , p 6 /( n in + n out )  as their initial values, where n in , n out stands for the number of neurons feeding in and neurons the result is fed to, respectively.",
235,pagesOfTextToDisplay-metadataToHighlight.json.json_235,"Besides, to prevent overfitting, we also add L2-regularization term in the loss function (Equation (5)), with the regularization amount adjusted to the best performance.",
236,pagesOfTextToDisplay-metadataToHighlight.json.json_236,"At reinforcement stage, the discount factor γ is set as 0.99.",
237,pagesOfTextToDisplay-metadataToHighlight.json.json_237,"We also apply some techniques mostly mentioned in [ 2 ] to reduce variance, including using an additional target Q-network and reward normalization.",
238,pagesOfTextToDisplay-metadataToHighlight.json.json_238,5.1.4 Comparison methods.,
239,pagesOfTextToDisplay-metadataToHighlight.json.json_239,"To demonstrate the effectiveness of STN, we compare our two implementations, i.e., STNM and STNR, with many state-of-the-art baselines as follows. •",
240,pagesOfTextToDisplay-metadataToHighlight.json.json_240,Enc-Dec is a plain encoder-decoder model used originally for image captioning [ 33 ].,
241,pagesOfTextToDisplay-metadataToHighlight.json.json_241,Its design allows it to be used in our problem setup with minor adjustments. •,
242,pagesOfTextToDisplay-metadataToHighlight.json.json_242,"Attn-Dot is an encoder-decoder model with attention mechanism following [ 22 ], where the attention score is calculated by directly computing the similarity between current output state and each encoded image vectors. •",
243,pagesOfTextToDisplay-metadataToHighlight.json.json_243,"Attn-FC is an encoder-decoder model similar to [ 33 ], but with basic visual attention strategy.",
244,pagesOfTextToDisplay-metadataToHighlight.json.json_244,"The model presents two attention strategies, i.e., the ""hard"" and ""soft"" attention mechanism, from which we follow [ 36 ] and choose the more widely used ""soft"" attention as it is deterministic and easier to train.",
245,pagesOfTextToDisplay-metadataToHighlight.json.json_245,"Research Track Paper KDD 2018, August 19 ‒ 23, 2018, London, United Kingdom 2649 KDD '18, August 19–23, 2018, London, United Kingdom Y. Yin et al.",
246,pagesOfTextToDisplay-metadataToHighlight.json.json_246,Table 2: Transcription accuracy on three datasets. (,
247,pagesOfTextToDisplay-metadataToHighlight.json.json_247,a) Melody Baseline Testing set percentage 40% 30% 20% 10% EncDec 0.266 0.272 0.277 0.282 AttnDot 0.524 0.548 0.580 0.617 AttnFC 0.683 0.710 0.730 0.756 AttnPos 0.725 0.736 0.741 0.758 STNM 0.729 0.733 0.749 0.759 STNR 0.738 0.748 0.758 0.767 (b) Formula Baseline Testing set percentage 40% 30% 20% 10% EncDec 0.405 0.427 0.445 0.451 AttnDot 0.530 0.563 0.600 0.611 AttnFC 0.657 0.701 0.717 0.725 AttnPos 0.716 0.723 0.732 0.741 STNM 0.717 0.726 0.740 0.749 STNR 0.739 0.751 0.759 0.778 (c) Multi-Line Baseline Testing set percentage 40% 30% 20% 10% EncDec 0.218 0.227 0.251 0.267 AttnDot 0.334 0.447 0.554 0.599 AttnFC 0.614 0.642 0.686 0.707 AttnPos 0.624 0.652 0.698 0.720 STNM 0.674 0.705 0.731 0.734 STNR 0.712 0.736 0.754 0.760 0 5 10 15 20 25 30 Epochs 0 .,
248,pagesOfTextToDisplay-metadataToHighlight.json.json_248,5 1 .,
249,pagesOfTextToDisplay-metadataToHighlight.json.json_249,0 1 .,
250,pagesOfTextToDisplay-metadataToHighlight.json.json_250,5 2 .,
251,pagesOfTextToDisplay-metadataToHighlight.json.json_251,0 2 .,
252,pagesOfTextToDisplay-metadataToHighlight.json.json_252,5 3 .,
253,pagesOfTextToDisplay-metadataToHighlight.json.json_253,0 Loss Enc-Dec Attn-Dot Attn-FC Attn-Pos STNM STNR (a) Melody 0 5 10 15 20 25 Epochs 0 .,
254,pagesOfTextToDisplay-metadataToHighlight.json.json_254,5 1 .,
255,pagesOfTextToDisplay-metadataToHighlight.json.json_255,0 1 .,
256,pagesOfTextToDisplay-metadataToHighlight.json.json_256,5 2 .,
257,pagesOfTextToDisplay-metadataToHighlight.json.json_257,0 2 .,
258,pagesOfTextToDisplay-metadataToHighlight.json.json_258,5 3 .,
259,pagesOfTextToDisplay-metadataToHighlight.json.json_259,0 Loss Enc-Dec Attn-Dot Attn-FC Attn-Pos STNM STNR (b) Formula 0 5 10 15 20 25 30 Epochs 0 .,
260,pagesOfTextToDisplay-metadataToHighlight.json.json_260,5 1 .,
261,pagesOfTextToDisplay-metadataToHighlight.json.json_261,0 1 .,
262,pagesOfTextToDisplay-metadataToHighlight.json.json_262,5 2 .,
263,pagesOfTextToDisplay-metadataToHighlight.json.json_263,0 2 .,
264,pagesOfTextToDisplay-metadataToHighlight.json.json_264,5 3 .,
265,pagesOfTextToDisplay-metadataToHighlight.json.json_265,0 Loss Enc-Dec Attn-Dot Attn-FC Attn-Pos STNM STNR (c) Multi-Line Figure 6: Validation loss of all models on three datasets. •,
266,pagesOfTextToDisplay-metadataToHighlight.json.json_266,"Attn-Pos is an encoder-decoder model designed specifically for scene text recognition [ 37 ], where besides the image content, it also embeds location information into attention calculation, and get superior results.",
267,pagesOfTextToDisplay-metadataToHighlight.json.json_267,"To conduct a fair comparison, the image encoders for baselines are changed to use the more recent ResNet [ 11 ] as our model does, with all of them tuned to have the best performance.",
268,pagesOfTextToDisplay-metadataToHighlight.json.json_268,"All models are implemented by PyTorch 4 , and trained on a Linux server with four 2.0GHz Intel Xeon E5-2620 CPUs and a Tesla K20m GPU.",
269,pagesOfTextToDisplay-metadataToHighlight.json.json_269,5.2 Experimental Results 5.2.1 Transcribing performance.,
270,pagesOfTextToDisplay-metadataToHighlight.json.json_270,"We train STN along with all the baseline models on four different data partition of each, comparing token accuracy at different data sparsity.",
271,pagesOfTextToDisplay-metadataToHighlight.json.json_271,We repeat all experiments 5 times and report the average results which are shown in Table 2.,
272,pagesOfTextToDisplay-metadataToHighlight.json.json_272,"From the results, we can get several observations.",
273,pagesOfTextToDisplay-metadataToHighlight.json.json_273,"First, both STNM and STNR perform better than all the other methods.",
274,pagesOfTextToDisplay-metadataToHighlight.json.json_274,"This indicates that STN framework is more capable for structural image transcription tasks, being more effective and accurate on tracking complex image structures.",
275,pagesOfTextToDisplay-metadataToHighlight.json.json_275,"Second, STN models, as well as attention based methods, all have much higher prediction accuracy than plain EncDec method, which proves the claim mentioned earlier in this paper that image information encoded as a single vector is confounding for decoder to decode, and both STN and attentive 4 http://pytorch.org models are able to reduce the confusion.",
276,pagesOfTextToDisplay-metadataToHighlight.json.json_276,"Moreover, STN models are consistently better than those attentive ones, showing the superiority of STN with separate modules for spotlighting and transcribing.",
277,pagesOfTextToDisplay-metadataToHighlight.json.json_277,"Third, STNR and STNM has slightly higher performance on Melody and Formula as Attn-Pos, but surpasses it marginally on Multi-Line dataset.",
278,pagesOfTextToDisplay-metadataToHighlight.json.json_278,"These results demonstrate that STN with spotlight mechanism can well preserve the internal structure of images, especially in more complex scenarios, benefiting the transcription accuracy.",
279,pagesOfTextToDisplay-metadataToHighlight.json.json_279,"Last but not least, we can see that STNR consistently outperforms than STNM, which indicates that it is effective to track long-term dependency for spotlighting in the process of transcribing structural image content.",
280,pagesOfTextToDisplay-metadataToHighlight.json.json_280,5.2.2 Validation loss.,
281,pagesOfTextToDisplay-metadataToHighlight.json.json_281,The losses of all models on the validation set throughout the training process on three datasets are shown in Figure 6.,
282,pagesOfTextToDisplay-metadataToHighlight.json.json_282,"There are also similar observations as before, which demonstrates the effectiveness of STN framework again.",
283,pagesOfTextToDisplay-metadataToHighlight.json.json_283,"Clearly, from the results, both STNR and STNM converge faster than the other models, and also achieve a lower loss.",
284,pagesOfTextToDisplay-metadataToHighlight.json.json_284,"Especially, the improvements of them on the more complex Multi-Line datasets are more significant.",
285,pagesOfTextToDisplay-metadataToHighlight.json.json_285,"Thus, we can reach a conclusion that STN with spotlight mechanism has superior ability to transcribe content from structural images.",
286,pagesOfTextToDisplay-metadataToHighlight.json.json_286,"Moreover, all models reach their lowest validation loss before 30 epochs, with STNR and STNM both come to their best point earlier.",
287,pagesOfTextToDisplay-metadataToHighlight.json.json_287,"Thus, in our experiments, we train both STNR and STNM for 25, 15, 20 epochs on Melody, Formula and Multi-Line datasets respectively to obtain the best performance.",
288,pagesOfTextToDisplay-metadataToHighlight.json.json_288,"Research Track Paper KDD 2018, August 19 ‒ 23, 2018, London, United Kingdom 2650 Transcribing Content from Structural Images with Spotlight Mechanism KDD '18, August 19–23, 2018, London, United Kingdom Ground Truth: dis 16 a ' [ b ...fis] dis16 a[ b c b]... dis 16 b ' [ c STNR Attn-Pos high low high low Figure 7: Comparison between attention and spotlight mechanism on Melody dataset.",
289,pagesOfTextToDisplay-metadataToHighlight.json.json_289,\frac { \sqrt { x \frac { \sqrt { x f(x)= \frac{\sqrt{x- 1}}{x-2} Ground Truth: - } STNR Attn-Pos high low high low Figure 8: Comparison between attention and spotlight mechanism on Formula dataset.,
290,pagesOfTextToDisplay-metadataToHighlight.json.json_290,5.2.3 Spotlight visualization.,
291,pagesOfTextToDisplay-metadataToHighlight.json.json_291,"To show the effectiveness of STN capturing the image structure and producing a reasonable reading path while transcribing, we visualize the spotlight weights computed by STNR when generating tokens, and compare them with the attention weights calculated by Attn-Pos model.",
292,pagesOfTextToDisplay-metadataToHighlight.json.json_292,"Figure 7 and Figure 8 visualize the results throughout image examples from Melody and Formula datasets, respectively.",
293,pagesOfTextToDisplay-metadataToHighlight.json.json_293,"5 In each example, we compare the attention and spotlight mechanism on how focused they are when generating a token, also on how well they track the image structure.",
294,pagesOfTextToDisplay-metadataToHighlight.json.json_294,"From the visualization, we can draw conclusions that: (1) STNR finds a more reasonable reading path on both examples.",
295,pagesOfTextToDisplay-metadataToHighlight.json.json_295,"In the melody example, it focuses on notes from left to right, and also tracks the height of each note, making accurate note pitch prediction; In the formula example, it clearly follows middle-top-bottom order when reading a fraction.",
296,pagesOfTextToDisplay-metadataToHighlight.json.json_296,"AttnPos model on the other hand, does not track the image structure well enough.",
297,pagesOfTextToDisplay-metadataToHighlight.json.json_297,"As shown in Figure 8, it fails to find the correct spot after generating "" \sqrt{x "", losing track of the radical expression, and generates the wrong token "" } "" at last. (",
298,pagesOfTextToDisplay-metadataToHighlight.json.json_298,"2) Although Attn-Pos model assigns more weights on content objects in images, e.g., notes, formulas and variables, it is often confused at areas with similar content.",
299,pagesOfTextToDisplay-metadataToHighlight.json.json_299,"On the other hand, STNR clearly distinguishes similar regions properly.",
300,pagesOfTextToDisplay-metadataToHighlight.json.json_300,"More specifically, in Figure 7, although Attn-Pos is able to focus on the notes, all notes are given similar weights as they look similar, which causes confusion and then 5 We only choose two real-world datasets for visualization due to the page limitation.",
301,pagesOfTextToDisplay-metadataToHighlight.json.json_301,wrong prediction.,
302,pagesOfTextToDisplay-metadataToHighlight.json.json_302,"And in Figure 8, when Attn-Pos writes x , three x 's in the image all have high weights, causing the model to forget where to look next.",
303,pagesOfTextToDisplay-metadataToHighlight.json.json_303,"On the contrary, STNR is well focused on the correct spot when generating each token on both of the datasets, which leads to more precise predictions.",
304,pagesOfTextToDisplay-metadataToHighlight.json.json_304,5.2.4 Discussion.,
305,pagesOfTextToDisplay-metadataToHighlight.json.json_305,All the above experiments have shown the effectiveness of STN on structural image transcription tasks.,
306,pagesOfTextToDisplay-metadataToHighlight.json.json_306,"It has superior performance on structural image transcription task compared to other general-purpose approaches, and also captures the structure of the image by producing a reading path following the image structure when transcribing.",
307,pagesOfTextToDisplay-metadataToHighlight.json.json_307,There are still some directions for further studies.,
308,pagesOfTextToDisplay-metadataToHighlight.json.json_308,"First, STN learns to transcribe tokens directly with little prior knowledge of the image or specific languages.",
309,pagesOfTextToDisplay-metadataToHighlight.json.json_309,"We are willing to utilize more prior knowledge, such as lexicons and hand-engineered features, to further improve the performance.",
310,pagesOfTextToDisplay-metadataToHighlight.json.json_310,"Second, we will try to apply our model to some more ambitious settings, such as transcribing with long-term context, also to make our model capable for other transcribing applications such as scene text recognition.",
311,pagesOfTextToDisplay-metadataToHighlight.json.json_311,"Third, we would like to further decouple the reading and writing process of STN, in order to mimic human behavior more genuinely.",
312,pagesOfTextToDisplay-metadataToHighlight.json.json_312,"6 CONCLUSION In this paper, we presented a novel hierarchical Spotlighted Transcribing Network (STN) for transcribing content from structural Research Track Paper KDD 2018, August 19 ‒ 23, 2018, London, United Kingdom 2651 KDD '18, August 19–23, 2018, London, United Kingdom Y. Yin et al.",
313,pagesOfTextToDisplay-metadataToHighlight.json.json_313,images by finding a reading path tracking the image internal structure.,
314,pagesOfTextToDisplay-metadataToHighlight.json.json_314,"Specifically, we first designed a two-stage ""where-to-what"" solution with a novel spotlight mechanism dedicated for the ""whereto-look"" problem, providing two implementations under the framework, modeling the spotlight movement through Markov chain and recurrent dependency, respectively.",
315,pagesOfTextToDisplay-metadataToHighlight.json.json_315,"Then, we applied supervised learning and reinforcement learning methods to accurately train and refine the spotlight modeling, in order to learn a reasonable reading path.",
316,pagesOfTextToDisplay-metadataToHighlight.json.json_316,"Finally, we conducted extensive experiments on one synthetic and two real-world datasets to demonstrate the effectiveness of STN framework with fast model convergence and high performance, and also visualized the learned reading path.",
317,pagesOfTextToDisplay-metadataToHighlight.json.json_317,We hope this work could lead to more studies in the future.,
